{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/ground_truth/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-50397d028c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcorrection_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mword_dict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCreate_Words_Dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCreate_Word_Pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Spring2019-Proj4-grp5-master/doc/correction_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mTesseract_Path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/tesseract/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mGround_Truth_Files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGet_FileNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGround_Truth_Path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mFile_Names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGround_Truth_Files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Spring2019-Proj4-grp5-master/doc/correction_lib.py\u001b[0m in \u001b[0;36mGet_FileNames\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mFile_Lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mground_truth_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mground_truth_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mFile_Lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/ground_truth/'"
     ]
    }
   ],
   "source": [
    "import correction_lib as corr\n",
    "from word_dict import Create_Words_Dictionary, Create_Word_Pair\n",
    "import string\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import collections\n",
    "import timeit\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a1648e6d7e67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgt_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ground_truth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgt_f\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgt_filenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "gt_filenames = glob.glob(os.path.join(os.getcwd(), 'data', 'ground_truth', '*.txt'))\n",
    "\n",
    "count = 0\n",
    "for gt_f in gt_filenames:\n",
    "    with open(gt_f) as file:\n",
    "        raw = file.read()\n",
    "        # Split file content into words (by '\\n', '\\t', ' ', etc.)\n",
    "        uncleaned_words = raw.split()\n",
    "        # Clean up words, leave only all-alpha chars of length > 1 (function programming)\n",
    "        word_list += list(filter(lambda x: 1 < len(x) < 21, map(clean_word, uncleaned_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3626ffc95d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgroup_by_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgroup_by_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# A dictionary of positional binary digrams (matrices),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "group_by_len = collections.defaultdict(list)\n",
    "for w in word_set:\n",
    "    group_by_len[len(w)].append(w)\n",
    "\n",
    "# A dictionary of positional binary digrams (matrices),\n",
    "# ordered by word length and then by binary positions\n",
    "digrams_by_len = collections.defaultdict(dict)\n",
    "for length in group_by_len:\n",
    "    for i, j in itertools.combinations(range(length), 2):\n",
    "        key = (i, j)\n",
    "        matrix = [[0] * 26 for _ in range(26)]\n",
    "        for w in group_by_len[length]:\n",
    "            matrix[char_to_index(w[i])][char_to_index(w[j])] = 1\n",
    "        digrams_by_len[length][key] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_word_list = []\n",
    "tr_filenames = glob.glob(os.path.join(os.getcwd(), 'data', 'tesseract', '*.txt'))\n",
    "for tr_f in tr_filenames:\n",
    "    with open(tr_f) as file:\n",
    "        raw = file.read()\n",
    "        uncleaned_words = raw.split()\n",
    "        tr_word_list += list(filter(lambda x: 1 < len(x) < 21, map(clean_word, uncleaned_words)))\n",
    "N_tr = len(tr_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_index(c):\n",
    "    if c==\"{\":\n",
    "        return(26)\n",
    "    else:\n",
    "        return(ord(c) - ord('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9a110bcfeed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#改动\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# read Confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0madd_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/confusion_matrix/add_matrix.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdel_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/confusion_matrix/del_matrix.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrev_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/confusion_matrix/rev_matrix.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#改动\n",
    "# read Confusion matrix\n",
    "add_matrix=np.array(pd.read_csv(\"./data/confusion_matrix/add_matrix.csv\",index_col=0))\n",
    "del_matrix=np.array(pd.read_csv(\"./data/confusion_matrix/del_matrix.csv\",index_col=0))\n",
    "rev_matrix=np.array(pd.read_csv(\"./data/confusion_matrix/rev_matrix.csv\",index_col=0))\n",
    "sub_matrix=np.array(pd.read_csv(\"./data/confusion_matrix/sub_matrix.csv\",index_col=0))\n",
    "\n",
    "# Useful values from section 3 of paper C-4.\n",
    "N = len(word_list)\n",
    "V = len(word_set)\n",
    "denominator = N + V/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correction_candidates(w):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    candidate_list = [[] for _ in range(4)]\n",
    "    \n",
    "    ### 4 kinds of correction candidates (see Table 2, C-4)\n",
    "    # 0. Deletion\n",
    "    for i in range(len(w) + 1):\n",
    "        for c in alphabet:\n",
    "            correction = w[:i] + c + w[i:]\n",
    "            if correction in word_set:\n",
    "                diff = corr.find_deletion_letters(correction, w) \n",
    "                candidate_list[0].append((correction, diff['pre_letter'], diff['delete_letter']))\n",
    "            \n",
    "    # 1. Insertion\n",
    "    for i in range(len(w)):\n",
    "        correction = w[:i] + w[i+1:]\n",
    "        if correction in word_set:\n",
    "            diff = corr.find_insertion_letters(correction, w) \n",
    "            candidate_list[1].append((correction, diff['pre_letter'], diff['insert_letter']))\n",
    "    \n",
    "    # 2. Substitution\n",
    "    for i in range(len(w)):\n",
    "        for c in alphabet:\n",
    "            if c != w[i]:\n",
    "                correction = w[:i] + c + w[i+1:]\n",
    "                if correction in word_set:\n",
    "                    diff = corr.find_sub_rev_letters(correction, w) \n",
    "                    if diff['tag'] == 'sub':\n",
    "                        candidate_list[2].append((correction, diff['pre_letter'], diff['changed_letter']))\n",
    "    \n",
    "    # 3. Reversal\n",
    "    for i in range(len(w) - 1):\n",
    "        correction = w[:i] + w[i+1] + w[i] + w[i+2:]\n",
    "        if correction in word_set:\n",
    "            diff = corr.find_sub_rev_letters(correction, w) \n",
    "            if diff['tag'] == 'rev':\n",
    "                candidate_list[3].append((correction, diff['pre_letter'], diff['changed_letter']))\n",
    "    \n",
    "    return(candidate_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1906582e24e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetected_error_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_candidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_correction_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "all_candidates = collections.defaultdict(dict)\n",
    "for word in detected_error_words:\n",
    "    all_candidates[word] = get_correction_candidates(word)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "print('Time:', stop - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = collections.defaultdict(int)\n",
    "for word in word_list:\n",
    "    word_freqs[word] += 1\n",
    "# print(dict((k, v) for k, v in word_freqs.items() if v >= 2))\n",
    "\n",
    "# Pr(c) of all possible corrections (all words from ground truth)\n",
    "word_freqs_freqs = {}\n",
    "for freqs in word_freqs.values():\n",
    "    if freqs in word_freqs_freqs.keys():\n",
    "        word_freqs_freqs[freqs] += 1\n",
    "    else:\n",
    "        word_freqs_freqs[freqs] = 1\n",
    "word_freqsN = {}\n",
    "for k in word_freqs_freqs.keys():\n",
    "    if k + 1  in word_freqs_freqs.keys():\n",
    "        word_freqs_freqsfinal[k+1] = word_freqs_freqs[k+1]\n",
    "    else:\n",
    "        word_freqs_freqsfinal[k+1] = 0.5\n",
    "N1 = 0\n",
    "for j in word_freqsN.keys():\n",
    "    N1 = N1 + j*word_freqsN[j]\n",
    "word_freqs_freqsfinal = {}\n",
    "for k in word_freqs_freqs.keys():\n",
    "    word_freqs_freqsfinal[k] = word_freqs_freqs[k]\n",
    "    if k + 1  in word_freqs_freqs.keys():\n",
    "        word_freqs_freqsfinal[k+1] = word_freqs_freqs[k+1]\n",
    "    else:\n",
    "        word_freqs_freqsfinal[k+1] = 0.5\n",
    "\n",
    "corr_probs_mle = collections.defaultdict(float)\n",
    "for word, freq in word_freqs.items():\n",
    "    corr_probs_mle[word] =freq/N\n",
    "corr_probs_ele = collections.defaultdict(float)\n",
    "for word, freq in word_freqs.items():\n",
    "    corr_probs[word] = (freq + 0.5)/denominator\n",
    "corr_probs_gt = collections.defaultdict(float)\n",
    "for word, freq in word_freqs.items():\n",
    "    corr_probs_gt[word] = (freq + 1)*word_freqs_freqsfinal[freqs + 1]/word_freqs_freqsfinal[freqs]/N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Pr_c(correction,method):\n",
    "    if method == \"MLe\":\n",
    "        return(corr_probs_mle[correction])\n",
    "    elif method == \"ELE\":\n",
    "        return(corr_probs_ele[correction])\n",
    "    else:\n",
    "        return(corr_probs_gt[correction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars[x] and chars[xy]\n",
    "# word_list=['abcdefg','awses']\n",
    "chars_x = np.zeros(26)\n",
    "chars_xy = np.zeros([27,26])\n",
    "print(chars_xy.shape)\n",
    "for word in word_list:\n",
    "    for i, c in enumerate(word):\n",
    "        chars_x[char_to_index(c)] += 1\n",
    "        if i==0:\n",
    "            chars_xy[26][char_to_index(c)] += 1\n",
    "        else:\n",
    "            chars_xy[char_to_index(word[i-1])][char_to_index(c)] += 1\n",
    "for i in range(len(chars_xy)):\n",
    "    for j in range(len(chars_xy[0])):\n",
    "        if not chars_xy[i][j]:\n",
    "            chars_xy[i][j] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Pr_tc(pre, cur, error_type):\n",
    "    if error_type == 'del':\n",
    "        return del_matrix[char_to_index(pre)][char_to_index(cur)] \\\n",
    "               / chars_xy[char_to_index(pre)][char_to_index(cur)]\n",
    "    if error_type == 'ins':\n",
    "        return ins_matrix[char_to_index(pre)][char_to_index(cur)] \\\n",
    "               / chars_x[char_to_index(pre)]\n",
    "    if error_type == 'sub':\n",
    "        return sub_matrix[char_to_index(pre)][char_to_index(cur)] \\\n",
    "               / chars_x[char_to_index(pre)]\n",
    "    if error_type == 'rev':\n",
    "        return rev_matrix[char_to_index(pre)][char_to_index(cur)] \\\n",
    "               / chars_xy[char_to_index(pre)][char_to_index(cur)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Pr_context_correction(cand, left, right):\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    # r_left = freq of left appearing, r_right = freq of right appearing\n",
    "    r_left, r_right = 0, 0\n",
    "    if left in neighbor_dict[cand]['left']:\n",
    "        r_left = neighbor_dict[cand]['left'][left]\n",
    "    if right in neighbor_dict[cand]['right']:\n",
    "        r_right = neighbor_dict[cand]['right'][right]\n",
    "        \n",
    "   \n",
    "        return(r_left*r_right)/N^2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Create_Word_Pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-88f47c968399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCreate_Word_Pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_corrections_found_tesseract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_letters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Create_Word_Pair' is not defined"
     ]
    }
   ],
   "source": [
    "result = Create_Word_Pair()\n",
    "word_pairs = result[:-1]\n",
    "num_corrections_found_tesseract, sum_letters = result[-1][0], result[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_Pr_correction(cand, typo, pre, cur, error_type, left, right, method):\n",
    "    return get_Pr_c(cand,method) * get_Pr_tc(typo, cand, pre, cur, error_type) \\\n",
    "           * get_Pr_context_correction(cand, left, right)\n",
    "\n",
    "err_type = ['del', 'ins', 'sub', 'rev']\n",
    "# A dictionary of the form: typo:(ground_truth, MLE_correction, ELE_correction)\n",
    "result = {}\n",
    "for typo, truth, left, right in detected_error_tuples2:\n",
    "    cand_poss_mle, cand_poss_ele,cand_poss_gt = [], [], []\n",
    "    for e in range(4):\n",
    "        for cand, pre, cur in all_candidates[typo][e]:\n",
    "            mle_p = get_final_Pr_correction(cand, typo, pre, cur, err_type[e], left, right, 'MLE')\n",
    "            ele_p = get_final_Pr_correction(cand, typo, pre, cur, err_type[e], left, right, 'ELE')\n",
    "            gt_p = get_final_Pr_correction(cand, typo, pre, cur, err_type[e], left, right, 'GT') \n",
    "            cand_poss_mle.append((cand, mle_p))\n",
    "            cand_poss_ele.append((cand, ele_p))\n",
    "            cand_poss_gt.append((cand, gte_p))\n",
    "    mle_best_cand = max(cand_poss_mle, key=lambda x: x[1])[0] if cand_poss_mle else ''\n",
    "    ele_best_cand = max(cand_poss_ele, key=lambda x: x[1])[0] if cand_poss_ele else ''\n",
    "    gt_best_cand = max(cand_poss_gt, key=lambda x: x[1])[0] if cand_poss_gt else ''\n",
    "    result[typo] = (truth, mle_best_cand, ele_best_cand,gt_best_cand)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
