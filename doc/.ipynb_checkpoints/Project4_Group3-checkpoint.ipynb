{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1YWiD2WA1fN"
   },
   "source": [
    "# Optical character recognition (OCR) \n",
    "Project 4 Group 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rViGVJ0xRK5u"
   },
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0c7XybVBJrI"
   },
   "source": [
    "First of all, we need to detect errors, or incorrectly processed words. Here we extract features according to the paper and use SVM for garbage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwpNpdFxUqll"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "#import itertools\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gw4gIzwQRDqJ",
    "outputId": "3616cd5c-2bfd-40c3-f14d-181a99982a77"
   },
   "outputs": [],
   "source": [
    "##### read ground_truth\n",
    "ground_dir = glob.glob(os.path.join(os.getcwd(),'../data/ground_truth','*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zoq2wOeFRDqQ"
   },
   "outputs": [],
   "source": [
    "# make sure corresponding files have the same number of lines\n",
    "tess_dir = glob.glob(os.path.join(os.getcwd(),'../data/tesseract','*.txt'))\n",
    "file_name_gd = []\n",
    "file_name_td = []\n",
    "for gd, td in zip(ground_dir, tess_dir):\n",
    "        with open(gd, encoding=\"utf8\") as ground_file:    #, encoding=\"utf8\"\n",
    "            with open(td, encoding=\"utf8\") as tess_file:                \n",
    "                ground_r = list(ground_file.readlines()) \n",
    "                tess_r = list(tess_file.readlines())\n",
    "                if len(tess_r) == len(ground_r):\n",
    "                    file_name_td.append(td)\n",
    "                    file_name_gd.append(gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rz5JErxMRDqU"
   },
   "outputs": [],
   "source": [
    "# make sure all the lines have the same size.\n",
    "# recreate ground_tokens and tess_tokens\n",
    "ground_tokens=[]\n",
    "tess_tokens=[]\n",
    "for gd, td in zip(file_name_gd, file_name_td):\n",
    "        with open(gd) as file1:\n",
    "            with open(td) as file2:\n",
    "                for line1,line2 in zip(file1,file2):\n",
    "                    if len(line1)==len(line2):\n",
    "                        #if length is different, skip that line\n",
    "                        for word1,word2 in zip(line1.split(),line2.split()):\n",
    "                            ground_tokens.append(word1)\n",
    "                            tess_tokens.append(word2)\n",
    "#ground_tokens and tess_tokens are a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xs2RfLUgXj6e",
    "outputId": "c4ace6b0-a38d-42a0-c5fe-0c412f0176eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communlcatlons', 'network.', 'Member', 'companles', 'are', 'strongly', 'encouraged', 'to', 'provlde', 'thls', 'needed', 'support.', 'The', 'state', 'advocacy', 'program\"', '1nclud1ng', 'the', 'new', 'CMA/LINC', 'computer', 'network.', 'will', 'be', 'heavlly', '1nvolved', '1n', '1995', '1n', 'the']\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC', 'computer', 'network,', 'will', 'be', 'heavily', 'involved', 'in', '1986', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(tess_tokens[:30])\n",
    "print(ground_tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IYwWhSBlRDqX",
    "outputId": "41f1bf1d-aad9-42e4-8b37-daaf9e123d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152143\n",
      "152143\n"
     ]
    }
   ],
   "source": [
    "print(len(tess_tokens))\n",
    "print(len(ground_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbgdRcXPRDqc"
   },
   "outputs": [],
   "source": [
    "# for every word in tess_tokens, y indicates the correctness of the word\n",
    "y = []\n",
    "for gt, tt in zip(ground_tokens, tess_tokens):\n",
    "        if gt == tt:\n",
    "            y.append(0)   # 0 indicates correct\n",
    "        else:\n",
    "            y.append(1)   # 1 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIxhIzlfUw7r"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tess_tokens)   \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "ground_train = [ground_tokens[i] for i in X_train.index.tolist()]\n",
    "ground_test = [ground_tokens[i] for i in X_test.index.tolist()]\n",
    "X_train = X_train[0].tolist()\n",
    "X_test = X_test[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "e6hxAoaAmSXD",
    "outputId": "5fa0fbc0-a836-4cea-e349-e9a31bd23eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['government', 'admlnlstered', 'commltment', 'price', 'and', 'Corporatlon', 'structure', 'technlcal', 'and', 'Nevertheless,', 'CMA.', 'develop', 'the', 'Inter', 'concerns', 'own', 'also', 'Reform', 'Hugh', 'of', 'further', 'treatment', 'atlng', 'for', '1992.', 'Voluntary', 'major', 'year.', 'fundlng', '15']\n",
      "['government', 'administered', 'commitment', 'price', 'and', 'Corporation', 'structure', 'technical', 'and', 'Nevertheless,', 'CMA.', 'develop', 'the', 'Intervene', 'concerns', 'own', 'also', 'Reform', 'Hugh', 'of', 'further', 'treatment', 'ating', 'for', '1982.', 'voluntary', 'major', 'year.', 'funding', 'is']\n",
      "[0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:30])\n",
    "print(ground_train[:30])\n",
    "print(y_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4xL-rP0tgm_"
   },
   "outputs": [],
   "source": [
    "# define character type\n",
    "vowels = list('aeiou')\n",
    "consonants = list(\"bcdfghjklmnpqrstvwxyz\")\n",
    "digits = list('0123456789')\n",
    "\n",
    "# Rule 9: If the word contains the # of upper case characters > # of lower case character => garbadge\n",
    "# count # of upper case characters \n",
    "def count_up(word):\n",
    "    upper = [c for c in word if c.isupper()]\n",
    "    return len(upper)\n",
    "# count # of lower case characters\n",
    "def count_low(word):\n",
    "    lower = [c for c in word if c.islower()]\n",
    "    return len(lower)\n",
    "# Rule 8: If the string contains at least 3 consecutive of same symbol => garbadge \n",
    "# count # consecutive occurrences of the same symbol\n",
    "def count_cons_occur_symbol(word): \n",
    "    count = 0 \n",
    "    tmp = 0 \n",
    "    curr = ''\n",
    "    for c in word:\n",
    "        if curr==c:\n",
    "            tmp +=1\n",
    "            else:\n",
    "                count = max(tmp,count)\n",
    "                tmp = 1 \n",
    "                curr = c\n",
    "    count = max(tmp, count)            \n",
    "    return count \n",
    "   \n",
    "# Rule 11: if the word contains 4 consecutive vowels or 5 consecutive consonants => garbage\n",
    "#consecutive occurrences of the same consonants\n",
    "def count_cons_occur_consonants(word):  \n",
    "    max_count=0\n",
    "    count = 0\n",
    "    curr = ''\n",
    "    for c in word:\n",
    "        if c in consonants:\n",
    "            if curr == c:\n",
    "                count += 1\n",
    "            else:\n",
    "                max_count = max(max_count, count)\n",
    "                curr = c\n",
    "                count = 1\n",
    "    max_count = max(max_count, count)\n",
    "    return(max_count)\n",
    "\n",
    "#extract a subset of word\n",
    "def trim_word(word, start=1, end=1):\n",
    "    return word[start:len(word) - end]\n",
    "\n",
    "def unique(list1):  \n",
    "    # intilize a null list \n",
    "    #unique_list = []      \n",
    "    # traverse for all elements \n",
    "    #for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        #if x not in unique_list: \n",
    "            #unique_list.append(x)\n",
    "    return(len(set(list1)))\n",
    "            \n",
    "def safe_div(x,y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y\n",
    "\n",
    "#return indexs of bigrams in word\n",
    "def get_bigram_freq(word, bi_dict):\n",
    "    word = word.lower()\n",
    "    bf = []\n",
    "    for i in range(len(word)-1):\n",
    "        key = word[i:i+2]\n",
    "        bf.append(bi_dict[key])\n",
    "    return(bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zd39iCOdEfom",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### find LB in ground_truth, prepared for bigram features\n",
    "# ground_tokens of all words in lowercase\n",
    "lower_ground_tokens = []\n",
    "for tk in ground_tokens:\n",
    "    tkl_g = tk.lower()\n",
    "    lower_ground_tokens.append(tkl_g)\n",
    "\n",
    "# A dict of all bigram frequencies\n",
    "bigram_dict = collections.defaultdict(int)\n",
    "for tk_g in lower_ground_tokens:\n",
    "    for i in range(len(tk_g)-1):\n",
    "        key = tk_g[i:i+2]\n",
    "        bigram_dict[key] += 1\n",
    "#bigram_dict is a dictionary of bigram frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dict['cd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ld7b_1PMDPn"
   },
   "outputs": [],
   "source": [
    "def feature_matrix(X):\n",
    "    #input: a list of words\n",
    "    #Output: a dataframe included features, every feature is a list\n",
    "    \n",
    "    ##### Feature extracting\n",
    "    # feature 1\n",
    "    length = []\n",
    "\n",
    "    # feature 2\n",
    "    vowels_count = []\n",
    "    consonants_count = []\n",
    "    quotients_v_l = []\n",
    "    quotients_c_l = []\n",
    "    quotients_v_c = []\n",
    "\n",
    "    # feature 3\n",
    "    nonalpha = []\n",
    "\n",
    "    # feature 4\n",
    "    digits = []\n",
    "    quotients_d_l = []\n",
    "\n",
    "    # feature 5\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    quotients_low_l = []\n",
    "    quotients_up_l = []\n",
    "    \n",
    "    #feature 6\n",
    "    consecutive_occur = []\n",
    "    quotients_cons_l = []\n",
    "    \n",
    "    #feature 7\n",
    "    quotients_la = [] # total number of vowels, consonants and digits\n",
    "    \n",
    "    #feature 8\n",
    "    consonants_secut = []\n",
    "    \n",
    "    #feature 9\n",
    "    fix_nonalpha = []\n",
    "\n",
    "    # feature bigram\n",
    "    bigr = []\n",
    "\n",
    "    #most frequent symbol\n",
    "    most_freq = []\n",
    "    \n",
    "    #non-alphabetical symbols\n",
    "    l2 = []\n",
    "    \n",
    "    for tk in X:\n",
    "        ### feature 1\n",
    "        l = len(tk)\n",
    "        length.append(l)\n",
    "\n",
    "        ### feature 2\n",
    "        tkl = tk.lower()\n",
    "        v_count = 0\n",
    "        for v in tkl:\n",
    "            if v in vowels:\n",
    "                v_count +=1\n",
    "        vowels_count.append(v_count)\n",
    "\n",
    "        c_count = 0\n",
    "        for c in tkl:\n",
    "            if c in consonants:\n",
    "                c_count +=1\n",
    "        consonants_count.append(c_count)\n",
    "\n",
    "        quotients_v_l.append(v_count/l)\n",
    "        quotients_c_l.append(c_count/l)\n",
    "        quot_v_c = safe_div(v_count,c_count)\n",
    "        quotients_v_c.append(quot_v_c)\n",
    "\n",
    "        ### feature 4\n",
    "        d_count = len([d for d in tk if d in digits])\n",
    "        digits.append(d_count)\n",
    "        quotients_d_l.append(d_count/l)\n",
    "\n",
    "        ### feature 3\n",
    "        s_count = len([s for s in tk if s not in vowels or consonants or digits])\n",
    "        nonalpha.append(s_count)\n",
    "\n",
    "        ### feature 5\n",
    "        low_count = count_low(tk)\n",
    "        lowers.append(low_count)\n",
    "        up_count = count_up(tk)\n",
    "        uppers.append(up_count)\n",
    "        quotients_low_l.append(low_count/l)\n",
    "        quotients_up_l.append(up_count/l)\n",
    "\n",
    "        ### feature 6\n",
    "        cons_occur_count = int(count_cons_occur_symbol(tk))\n",
    "        consecutive_occur.append(cons_occur_count)\n",
    "        if cons_occur_count >= 3:\n",
    "            quotients_cons_l.append(cons_occur_count/l)\n",
    "        else:\n",
    "            quotients_cons_l.append(0)\n",
    "\n",
    "        ### feature 7\n",
    "        la = v_count + c_count + d_count\n",
    "        if s_count > la:\n",
    "            quotients_la.append(1)\n",
    "        else:\n",
    "            quotients_la.append(0)\n",
    "\n",
    "        ### feature 8\n",
    "        consonance_cons_cccur_count = int(count_cons_occur_consonants(tk))\n",
    "        if consonance_cons_cccur_count >= 6:\n",
    "            consonants_secut.append(1)\n",
    "        else:\n",
    "            consonants_secut.append(0)\n",
    "\n",
    "        ### feature 9\n",
    "        tk_removed = trim_word(tk)\n",
    "        k_count = len([k for k in tk_removed if k not in vowels or consonants or digits])\n",
    "        if k_count >=3:\n",
    "            fix_nonalpha.append(1)\n",
    "        else:\n",
    "            fix_nonalpha.append(0)  \n",
    "\n",
    "        ### feature 10 bigram\n",
    "        bf = get_bigram_freq(tk, bigram_dict)\n",
    "\n",
    "        lower_tess_tokens = []\n",
    "        tkl_t = tk.lower()\n",
    "        lower_tess_tokens.append(tkl_t)\n",
    "\n",
    "        n = len(set(lower_tess_tokens))\n",
    "        big = (sum(bf)/10000)/n \n",
    "        bigr.append(big)\n",
    "\n",
    "        ### feature 11 most frequent symbol\n",
    "        i_count = Counter(tk).most_common(1)[0][1]\n",
    "        if i_count >=3:\n",
    "            most_freq.append(1)\n",
    "        else:\n",
    "            most_freq.append(0)\n",
    "\n",
    "        ### feature 12 Non-alphabetical symbols: nonalpha/total symbols\n",
    "        l1 = len([v for v in tk.lower() if v in vowels] + [c for c in tk.lower() if c in consonants])\n",
    "        l2_count = l - l1\n",
    "        quot_l2 = safe_div(l2_count,l1)\n",
    "        l2.append(quot_l2)  \n",
    "        \n",
    "    ##### construct a feature dataframe for SVM\n",
    "    df1 = pd.DataFrame({'length': length,\n",
    "                        'vowels': vowels_count,\n",
    "                        'consonants': consonants_count,\n",
    "                        'quot v/l': quotients_v_l,\n",
    "                        'quot c/l': quotients_c_l,\n",
    "                        'quot v/c': quotients_v_c,\n",
    "                        'nonalpha': nonalpha,\n",
    "                        'digits': digits,\n",
    "                        'quot d/l': quotients_d_l,\n",
    "                        'lowers': lowers,\n",
    "                        'uppers': uppers,\n",
    "                        'quot low/l': quotients_low_l,\n",
    "                        'quot up/l': quotients_up_l,\n",
    "                        'cons occur': consecutive_occur,\n",
    "                        'quot cons/l': quotients_cons_l,\n",
    "                        'quot la': quotients_la,\n",
    "                        'consonants_secut': consonants_secut,\n",
    "                        'fix_nonalpha': fix_nonalpha,\n",
    "                        'bigr': bigr,\n",
    "                        'most_freq' : most_freq,\n",
    "                        'l2': l2})\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4qm7cN9RDqv"
   },
   "outputs": [],
   "source": [
    "#it takes about 10 mins\n",
    "X_feature_train = feature_matrix(X_train)\n",
    "X_feature_test = feature_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature_train.to_csv('../output/X_feature_train')\n",
    "X_feature_test.to_csv('../output/X_feature_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "VD1UsNAORDq0",
    "outputId": "34b6487b-0fba-43ae-f87b-271c4e046e5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it takes about 20 mins\n",
    "svclassifier = SVC(kernel='rbf')  \n",
    "svclassifier.fit(X_feature_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import pickle\n",
    "filename = '../output/SVM_model.sav'\n",
    "pickle.dump(svclassifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this chunk is for load model\n",
    "import pickle\n",
    "filename = '../output/SVM_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSwqJ770RDq4"
   },
   "outputs": [],
   "source": [
    "#to predict\n",
    "y_pred = svclassifier.predict(X_feature_test)  \n",
    "# y_pred = svm_model.predict(X_feature_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1127
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XGh8N69KRDq8",
    "outputId": "f592fa21-538d-44c6-baaf-d065761ff038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tokens_tesseract  Predict_by_SVM\n",
      "0            Unlted               0\n",
      "1             EPA'S               1\n",
      "2           because               0\n",
      "3            Signed               0\n",
      "4         commlttee               1\n",
      "5               the               0\n",
      "6           percent               0\n",
      "7        accldental               1\n",
      "8              ppm,               1\n",
      "9               the               0\n",
      "10                5               1\n",
      "11               21               1\n",
      "12          'Senate               0\n",
      "13              the               0\n",
      "14              the               0\n",
      "15              The               0\n",
      "16             th.t               1\n",
      "17               at               0\n",
      "18        Congress,               0\n",
      "19              the               0\n",
      "20             Food               1\n",
      "21              EPA               0\n",
      "22         However.               0\n",
      "23              gas               0\n",
      "24               10               1\n",
      "25           actlon               0\n",
      "26            â€™anal               0\n",
      "27           report               0\n",
      "28            staff               1\n",
      "29               at               0\n",
      "..              ...             ...\n",
      "70           state,               0\n",
      "71              and               0\n",
      "72              the               0\n",
      "73    demonstratlon               1\n",
      "74               of               0\n",
      "75               M.               0\n",
      "76               1:               1\n",
      "77               L.               0\n",
      "78               or               0\n",
      "79              and               0\n",
      "80             good               0\n",
      "81               or               0\n",
      "82              Bra               0\n",
      "83          fortlgn               1\n",
      "84     governments.               1\n",
      "85               of               0\n",
      "86              and               0\n",
      "87              who               0\n",
      "88              and               0\n",
      "89         Industry               1\n",
      "90             make               0\n",
      "91              the               0\n",
      "92          flgures               0\n",
      "93           needed               0\n",
      "94               of               0\n",
      "95        contacts.               1\n",
      "96       actlvltles               1\n",
      "97          example               0\n",
      "98        synthetlc               1\n",
      "99            needs               0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_output = pd.DataFrame({'tokens_tesseract':X_test,\n",
    "                          'Predict_by_SVM': y_pred})\n",
    "print(df_output[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "LgJ-6B2tRDrC",
    "outputId": "e4512c20-ce44-4c10-e203-e21bba73e5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16269  2140]\n",
      " [ 3491  8529]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.88      0.85     18409\n",
      "          1       0.80      0.71      0.75     12020\n",
      "\n",
      "avg / total       0.81      0.81      0.81     30429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLP performs bad\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "clf.fit(X_feature_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18372     0]\n",
      " [12057     0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      1.00      0.75     18372\n",
      "          1       0.00      0.00      0.00     12057\n",
      "\n",
      "avg / total       0.36      0.60      0.45     30429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenxishi/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = '../output/MLP_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "y_pred = clf.predict(X_feature_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(X_feature_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17526   846]\n",
      " [ 2382  9675]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.95      0.92     18372\n",
      "          1       0.92      0.80      0.86     12057\n",
      "\n",
      "avg / total       0.90      0.89      0.89     30429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = '../output/KNN_model.sav'\n",
    "pickle.dump(neigh, open(filename, 'wb'))\n",
    "\n",
    "y_pred = neigh.predict(X_feature_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenxishi/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(n_estimators=10,min_samples_split=2, random_state=0)\n",
    "RF = RF.fit(X_feature_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = '../output/RF_model.sav'\n",
    "pickle.dump(neigh, open(filename, 'wb'))\n",
    "\n",
    "y_pred = neigh.predict(X_feature_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7ezVSAnaity"
   },
   "source": [
    "## Error correction(I didn't run this part--Xishi Chen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CktPwLtalAr"
   },
   "source": [
    "Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates. Here, we implement the positional binary digram method in the first reference paper. (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564}{positional binary digram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VEfK86-1Dd5"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fn7sOOYDapFz"
   },
   "outputs": [],
   "source": [
    "def keep_alphabet(tokens):\n",
    "  # only retain alphabet\n",
    "  out = []\n",
    "  for l in tokens:\n",
    "      l = l.lower()\n",
    "      if l in set('abcdefghijklmnopqrstuvwxyz '):\n",
    "          out.append(l)\n",
    "  return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOWpBGeSeAzz"
   },
   "outputs": [],
   "source": [
    "# Replace a postion of a string\n",
    "def replace_str_index(text,index=0,replacement=''):\n",
    "    return '%s%s%s'%(text[:index],replacement,text[index+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBwd4B3oh6LL"
   },
   "outputs": [],
   "source": [
    "# Define the main correction function\n",
    "def correction(word, digrams):\n",
    "  detect = 0\n",
    "  beta = []\n",
    "  # Find the error positions\n",
    "  for each in digrams:\n",
    "    matrix = digrams[each]\n",
    "    if matrix[string.ascii_lowercase.index(word[each[0]])][string.ascii_lowercase.index(word[each[1]])] == 0:\n",
    "      detect += 1\n",
    "      beta_each = set(each)\n",
    "      if detect == 1:\n",
    "        beta = beta_each\n",
    "      else:\n",
    "        beta = beta.intersection(beta_each)\n",
    "  # print(beta)\n",
    "  # Consider when there is one or two elements in beta\n",
    "  if len(beta) in [1,2]:\n",
    "    choices = 0\n",
    "    v_dict = {}\n",
    "    for i in beta:\n",
    "      v_list = []\n",
    "      position = i\n",
    "      for j in range(len(word)):\n",
    "        alpha_j = string.ascii_lowercase.index(word[j])\n",
    "        if j < position:\n",
    "          vector_j = digrams[(j, position)][alpha_j]\n",
    "          v_list.append(vector_j)\n",
    "        elif j > position:\n",
    "          vector_j = [item[alpha_j] for item in digrams[(position, j)]]\n",
    "          v_list.append(vector_j)\n",
    "      v = v_list[0]\n",
    "      for each in v_list:\n",
    "        v = [a and b for a, b in zip(v, each)]\n",
    "      if sum(v) == 1:\n",
    "        choices += 1\n",
    "        v_dict[i] = v\n",
    "    if choices == 1:\n",
    "      for key in v_dict:\n",
    "        position_chosen = key\n",
    "      word = replace_str_index(word, position_chosen, string.ascii_lowercase[v_dict[position_chosen].index(1)])\n",
    "  return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vr1Bsyh1cPUL"
   },
   "outputs": [],
   "source": [
    "for i in range(len(ground_train)):\n",
    "  ground_train[i] = keep_alphabet(ground_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KztP5VHZcN2J"
   },
   "outputs": [],
   "source": [
    "# Create dictionary of digrams\n",
    "token_by_len = collections.defaultdict(list)\n",
    "digrams_by_len = collections.defaultdict(dict)\n",
    "for w in ground_train:\n",
    "  token_by_len[len(w)].append(w)\n",
    "  \n",
    "#print('Number of words of diffenrent length:')\n",
    "#for key, value in token_by_len.items() :\n",
    "#    print (key, len(value))\n",
    "\n",
    "for length in token_by_len:\n",
    "  for i, j in itertools.combinations(range(length), 2):\n",
    "    key = (i, j)\n",
    "    matrix = [[0] * 26 for _ in range(26)]\n",
    "    for words in token_by_len[length]:\n",
    "      matrix[string.ascii_lowercase.index(words[i])][string.ascii_lowercase.index(words[j])] = 1\n",
    "    digrams_by_len[length][key] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KBWvfZZbVQCd",
    "outputId": "37ca3d07-b0b8-43e7-b024-43ed70a60e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "# Number of 1 in digrams with given length\n",
    "ae = 0\n",
    "for each in digrams_by_len[10][(0,1)]:\n",
    "  ae += sum(each)\n",
    "print(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEmgfeS4KP5R"
   },
   "outputs": [],
   "source": [
    "# Clean the words\n",
    "corrected_test = X_test.copy()\n",
    "for i in range(len(corrected_test)):\n",
    "  corrected_test[i] = keep_alphabet(corrected_test[i])\n",
    "for i in range(len(ground_test)):\n",
    "  ground_test[i] = keep_alphabet(ground_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TSJth-EgTOb"
   },
   "outputs": [],
   "source": [
    "# Make correction\n",
    "for i in range(len(y_pred)):\n",
    "  if y_pred[i] == 1:\n",
    "    word_length = len(corrected_test[i])\n",
    "    if word_length > 1:\n",
    "      digrams_i = digrams_by_len[word_length]\n",
    "      corrected_test[i] = correction(corrected_test[i], digrams_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "towmW8Vkuj3-"
   },
   "outputs": [],
   "source": [
    "y_corrected = []\n",
    "for gt, ct in zip(ground_test, corrected_test):\n",
    "        if gt == ct:\n",
    "            y_corrected.append(0)   # 0 indicates correct\n",
    "        else:\n",
    "            y_corrected.append(1)   # 1 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "P2uE-TeO7bQX",
    "outputId": "57b2e4a2-e157-4aeb-e8ec-52df581fcb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'on', 'and', 'washlngton', '', '', 'report', '', 'reached', 'could', 'cozporatlon', 'the', 'recent', 'vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'mr', 'addltlonal', 'to', 'approach', 'a', 'commlttee', 'are', 'would', 'but']\n",
      "['in', 'on', 'and', 'washington', '', 'it', 'report', '', 'reached', 'could', 'corporation', 'the', 'recent', 'vent', 'session', 'are', 'and', 'able', 'governments', 'commodity', 'the', 'mr', 'additional', 'to', 'approach', 'a', 'committee', 'are', 'would', 'but']\n",
      "['1n', 'on', 'and', 'Washlngton,', '7', '1:', 'Report', '40', 'reached', 'could', 'Cozporatlon', 'the', 'recent', 'Vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'Mr.', 'Addltlonal', 'to', 'approach', 'a', 'Commlttee', 'are:', 'would', 'but']\n"
     ]
    }
   ],
   "source": [
    "# Compare the results\n",
    "print(corrected_test[:30])\n",
    "print(ground_test[:30])\n",
    "print(X_test[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RathHlusCCmv",
    "outputId": "0357315b-8faa-4676-abf4-1093d94898c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12039\n",
      "10362\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_test))\n",
    "print(sum(y_corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVSjkKFiAwdQ"
   },
   "source": [
    "## Performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8QLnVn18-k7"
   },
   "outputs": [],
   "source": [
    "# Create corresponding lists of characters\n",
    "corrected_char = []\n",
    "for each in corrected_test:\n",
    "  corrected_char += each\n",
    "corrected_char\n",
    "\n",
    "ground_char = []\n",
    "for each in ground_test:\n",
    "  ground_char += each\n",
    "\n",
    "tess_char = [] \n",
    "tess_test = X_test.copy()\n",
    "for i in range(len(tess_test)):\n",
    "  tess_test[i] = keep_alphabet(tess_test[i])\n",
    "for each in tess_test:\n",
    "  tess_char += each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RN0kdSp_RoYM"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7swhDLPT-EZK"
   },
   "outputs": [],
   "source": [
    "# Count the number of characters\n",
    "ground_count = []\n",
    "corrected_count = []\n",
    "tess_count = []\n",
    "for each in string.ascii_lowercase:\n",
    "  ground_count.append(ground_char.count(each))\n",
    "  corrected_count.append(corrected_char.count(each))\n",
    "  tess_count.append(tess_char.count(each))\n",
    "ground_corrected_min = np.minimum(ground_count, corrected_count)\n",
    "ground_tess_min = np.minimum(ground_count, tess_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "iH01NQqpAowo",
    "outputId": "b2436256-1bf0-447e-b71d-28f14b99f950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Tesseract  Tesseract_with_postprocessing\n",
      "word_wise_recall           0.604358                       0.659470\n",
      "word_wise_precision        0.604358                       0.659470\n",
      "character_wise_recall      0.912326                       0.917584\n",
      "character_wise_precision   0.945590                       0.951040\n"
     ]
    }
   ],
   "source": [
    "# Make the evaluation table\n",
    "word_recall_tess = 1 - sum(y_test)/len(ground_test)\n",
    "word_recall_corrected = 1 - sum(y_corrected)/len(ground_test)\n",
    "word_precision_tess = 1 - sum(y_test)/len(X_test)\n",
    "word_precision_corrected = 1 - sum(y_corrected)/len(X_test)\n",
    "char_recall_tess = sum(ground_tess_min)/sum(ground_count)\n",
    "char_precision_tess = sum(ground_tess_min)/sum(tess_count)\n",
    "char_recall_corrected = sum(ground_corrected_min)/sum(ground_count)\n",
    "char_precision_corrected = sum(ground_corrected_min)/sum(corrected_count)\n",
    "\n",
    "d = {'Tesseract': [word_recall_tess, word_precision_tess, char_recall_tess, char_precision_tess], \n",
    "     'Tesseract_with_postprocessing': [word_recall_corrected, word_precision_corrected \n",
    "                                       ,char_recall_corrected, char_precision_corrected]}\n",
    "OCR_performance_table = pd.DataFrame(data=d)\n",
    "OCR_performance_table.rename(index={0: 'word_wise_recall', 1:'word_wise_precision', \n",
    "                                    2:'character_wise_recall', 3:'character_wise_precision'}, inplace=True)\n",
    "print(OCR_performance_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHvcr4LMA0VQ"
   },
   "source": [
    "The word-wise recall after postprocessing is 0.6595, significantly better than original Tesseract text. Word-wise precision is the same as recall because of our preprocessing. The character-wise recall and precision are slightly improved to 0.9176 and 0.951."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project4_Detection_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
