{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1YWiD2WA1fN"
   },
   "source": [
    "# Optical character recognition (OCR) \n",
    "Project 4 Group 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rViGVJ0xRK5u"
   },
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0c7XybVBJrI"
   },
   "source": [
    "First of all, we need to detect errors, or incorrectly processed words. Here we extract features according to the paper and use SVM for garbage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwpNpdFxUqll"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "#import itertools\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gw4gIzwQRDqJ",
    "outputId": "3616cd5c-2bfd-40c3-f14d-181a99982a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294841\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC']\n"
     ]
    }
   ],
   "source": [
    "##### read ground_truth\n",
    "ground_dir = glob.glob(os.path.join(os.getcwd(),'../data/ground_truth','*.txt'))\n",
    "ground_tokens = []\n",
    "for gd in ground_dir:\n",
    "    with open(gd) as ground_file:\n",
    "        ground_raw = ground_file.read()\n",
    "        ground_t = ground_raw.split()   \n",
    "        ground_tokens += ground_t\n",
    "print(len(ground_tokens))\n",
    "print(ground_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zoq2wOeFRDqQ"
   },
   "outputs": [],
   "source": [
    "tess_dir = glob.glob(os.path.join(os.getcwd(),'../data/tesseract','*.txt'))\n",
    "#ground_dir = glob.glob(os.path.join(os.getcwd(),'data/ground_truth','*.txt'))\n",
    "file_name_gd = []\n",
    "file_name_td = []\n",
    "for gd, td in zip(ground_dir, tess_dir):\n",
    "        with open(gd, encoding=\"utf8\") as ground_file:    #, encoding=\"utf8\"\n",
    "            with open(td, encoding=\"utf8\") as tess_file:                \n",
    "                ground_r = list(ground_file.readlines()) \n",
    "                tess_r = list(tess_file.readlines())\n",
    "                if len(tess_r) == len(ground_r):\n",
    "                    file_name_td.append(td)\n",
    "                    file_name_gd.append(gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rz5JErxMRDqU"
   },
   "outputs": [],
   "source": [
    "# make sure all the lines have the same size.\n",
    "ground_tokens=[]\n",
    "tess_tokens=[]\n",
    "for gd, td in zip(file_name_gd, file_name_td):\n",
    "        with open(gd) as file1:\n",
    "            with open(td) as file2:\n",
    "                for line1,line2 in zip(file1,file2):\n",
    "                    if len(line1)==len(line2):\n",
    "                        for word1,word2 in zip(line1.split(),line2.split()):\n",
    "                            ground_tokens.append(word1)\n",
    "                            tess_tokens.append(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xs2RfLUgXj6e",
    "outputId": "c4ace6b0-a38d-42a0-c5fe-0c412f0176eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communlcatlons', 'network.', 'Member', 'companles', 'are', 'strongly', 'encouraged', 'to', 'provlde', 'thls', 'needed', 'support.', 'The', 'state', 'advocacy', 'program\"', '1nclud1ng', 'the', 'new', 'CMA/LINC', 'computer', 'network.', 'will', 'be', 'heavlly', '1nvolved', '1n', '1995', '1n', 'the']\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC', 'computer', 'network,', 'will', 'be', 'heavily', 'involved', 'in', '1986', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(tess_tokens[:30])\n",
    "print(ground_tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IYwWhSBlRDqX",
    "outputId": "41f1bf1d-aad9-42e4-8b37-daaf9e123d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152143\n",
      "152143\n"
     ]
    }
   ],
   "source": [
    "print(len(tess_tokens))\n",
    "print(len(ground_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbgdRcXPRDqc"
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "for gt, tt in zip(ground_tokens, tess_tokens):\n",
    "        if gt == tt:\n",
    "            y.append(0)   # 0 indicates correct\n",
    "        else:\n",
    "            y.append(1)   # 1 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIxhIzlfUw7r"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tess_tokens)   \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "ground_train = [ground_tokens[i] for i in X_train.index.tolist()]\n",
    "ground_test = [ground_tokens[i] for i in X_test.index.tolist()]\n",
    "X_train = X_train[0].tolist()\n",
    "X_test = X_test[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "e6hxAoaAmSXD",
    "outputId": "5fa0fbc0-a836-4cea-e349-e9a31bd23eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['esent', 'of', 'for', 'tax', 'Superfund', 'present', 'changes', 'exposure', '11m:', 'Congress', 'Â£111', '1n', '1991792', 'hazardous', 'the', '15', 'of', '15', 'members.', \"1ndustry's\", '35515:', 'protectlon', '1dent1ty', 'progress', 'an', 'The', 'classes', 'he', 'on', 'Vlew']\n",
      "['non-degredation', 'of', 'for', 'tax', 'Superfund', 'present', 'changes', 'exposure', 'zinc', 'Congress', 'fill', 'in', '1981-82', 'hazardous', 'the', 'is', 'of', 'is', 'members.', \"industry's\", 'assist', 'protection', 'identity', 'progress', 'an', 'The', 'classes', 'he', 'on', 'view']\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:30])\n",
    "print(ground_train[:30])\n",
    "print(y_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4xL-rP0tgm_"
   },
   "outputs": [],
   "source": [
    "# define character type\n",
    "vowels = list('aeiou')\n",
    "consonants = list(\"bcdfghjklmnpqrstvwxyz\")\n",
    "digits = list('0123456789')\n",
    "\n",
    "def count_up(word):\n",
    "    u = [x for x in word if x.isupper()]\n",
    "    return len(u)\n",
    "\n",
    "def count_low(word):\n",
    "    l = [x for x in word if x.islower()]\n",
    "    return len(l)\n",
    "\n",
    "# count the number of consecutive of the same symbol (if greater than 3 => garbadge)\n",
    "def count_cons_occur(word):\n",
    "    max_occ=0\n",
    "    previous=None\n",
    "    temp=1\n",
    "    for i in word:\n",
    "        if i==previous:\n",
    "            temp=temp+1\n",
    "        else:\n",
    "            max_occ=max(max_occ,temp)\n",
    "            temp=1\n",
    "        previous=i\n",
    "    return(max_occ)\n",
    "\n",
    "#  count the number of consecutive of the consonants(if greater than 6 => garbadge)\n",
    "def count_cons_occur_consonants(word):\n",
    "    max_occ=0\n",
    "    temp=0\n",
    "    for i in word:\n",
    "        if i in consonants:\n",
    "            temp=temp+1\n",
    "        else:\n",
    "            max_occ=max(max_occ,temp)\n",
    "            temp=0\n",
    "    return(max_occ)\n",
    "\n",
    "# get the subset of the word: delete the 1st and last word \n",
    "def subset_word(word):\n",
    "    return word[1:len(word) - 1]\n",
    "\n",
    "#  do the division in case of  prevent from divisor = 0        \n",
    "def div(x,y):\n",
    "    if y != 0:\n",
    "        return x/y\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zd39iCOdEfom",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the bigram frequency for the particular word (return the array of each bf_i)\n",
    "def bigram_freq(word, bi_dict):\n",
    "    word = word.lower()\n",
    "    bf = []\n",
    "    for i in range(len(word)-1):\n",
    "        key = word[i:i+2]\n",
    "        bf.append(bi_dict[key])\n",
    "    return(bf)\n",
    "\n",
    "# Form the list of letter bigrams from our ground truth dataset -- bi_dict\n",
    "# First we normalized all tokens to the lowercase \n",
    "lower_case_words = []\n",
    "for word in ground_tokens:\n",
    "    lower_word = word.lower()\n",
    "    lower_case_words.append(lower_word)\n",
    "\n",
    "# Secondly, we form the LB from our ground_truth dataset \n",
    "# create a dictionary-like collection: key: bigrams value: frequency \n",
    "LB = collections.defaultdict(int)\n",
    "for word in lower_case_words:\n",
    "    for i in range(len(word)-1):\n",
    "        key = word[i:i+2]\n",
    "        LB[key] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ld7b_1PMDPn"
   },
   "outputs": [],
   "source": [
    "def feature_matrix(data):\n",
    "    ##### Feature extracting\n",
    "    \n",
    "    # feature 1 : length of the each words\n",
    "    length = []\n",
    "\n",
    "    # feature 2: number of each vowels and consonants; and the corresponding ratio\n",
    "    vowels_count = []\n",
    "    consonants_count = []\n",
    "    ratio_v_l = []\n",
    "    ratio_c_l = []\n",
    "    ratio_v_c = []\n",
    "\n",
    "    # feature 3: number of special symbols: not vowels, not consonants and not digits \n",
    "    nonalpha = []\n",
    "\n",
    "    # feature 4: number of digits and the ratio bewteen number of digit and length of the words \n",
    "    digits = []\n",
    "    ratio_d_l = []\n",
    "\n",
    "    # feature 5: number of lowercase chars, uppercase chars and corresponding ratio \n",
    "    lower_case = []\n",
    "    upper_case = []\n",
    "    ratio_low_l = []\n",
    "    ratio_up_l = []\n",
    "    \n",
    "    # feature 6: number of consecutive occurances of the same symbol and the ratio between it and the length\n",
    "    consecutive_occur = []\n",
    "    ratio_sym_l = []\n",
    "    \n",
    "    # feature 7: number of all alpha(words) and numbers in the words and the ratio between it and the length\n",
    "    # total number of vowels, consonants and digits\n",
    "    ratio_la = [] \n",
    "    \n",
    "    # feature 8: number of consecutive consonants in the word\n",
    "    consonants_consecutive = []\n",
    "    \n",
    "    # feature 9: delete the 1st and the last symbol and then check the number of non alpha - number  character \n",
    "    infix_nonalpha = []\n",
    "\n",
    "    # feature 10: bigram frequency \n",
    "    bigram = []\n",
    "    \n",
    "    # feature 11: number of most frequent symbols (including alpha,numbers, special symbols)\n",
    "    most_freq = []\n",
    "    \n",
    "    # feature 12: number of non-alpha symbols: length(words) - length(alpha)\n",
    "    l2 = []\n",
    "    \n",
    "    \n",
    "        # feature 1:\n",
    "    for word in data:\n",
    "        L= len(word)\n",
    "        length.append(L)\n",
    "\n",
    "        # feature 2: vowels\n",
    "        word_lower = word.lower()\n",
    "        v_count = len([v for v in word if v in vowels])\n",
    "        vowels_count.append(v_count)\n",
    "        \n",
    "        # feature 2: consonants\n",
    "        c_count = len([c for c in word if c in consonants])\n",
    "        consonants_count.append(c_count)\n",
    "   \n",
    "        ratio_v_l.append(v_count/L)\n",
    "        ratio_c_l.append(c_count/L)\n",
    "        ratio_vc = div(v_count,c_count)\n",
    "        ratio_v_c.append(ratio_vc)\n",
    "\n",
    "        # feature 3\n",
    "        symbol_count = len([s for s in word if s not in vowels or consonants or digits])\n",
    "        nonalpha.append(symbol_count)\n",
    "     \n",
    "        # feature 4\n",
    "        d_count = len([d for d in word if d in digits])\n",
    "        digits.append(d_count)\n",
    "        ratio_d_l.append(d_count/L)\n",
    "        \n",
    "        # feature 5\n",
    "        low_count = count_low(word)\n",
    "        lower_case.append(low_count)\n",
    "        upper_count = count_up(word)\n",
    "        upper_case.append(upper_count)\n",
    "\n",
    "        ratio_low_l.append(low_count/L)\n",
    "        ratio_up_l.append(upper_count/L)\n",
    "\n",
    "        # feature 6\n",
    "        cons_occur_count = int(count_cons_occur(word))\n",
    "        consecutive_occur.append(cons_occur_count)\n",
    "        if cons_occur_count >= 3:\n",
    "            ratio_sym_l.append(cons_occur_count/L)\n",
    "        else:\n",
    "            ratio_sym_l.append(0)\n",
    "\n",
    "        # feature 7\n",
    "        la = L-symbol_count\n",
    "        if symbol_count > la:\n",
    "            ratio_la.append(1)\n",
    "        else:\n",
    "            ratio_la.append(0)\n",
    "\n",
    "        # feature 8\n",
    "        consonance_consecu_count = int(count_cons_occur_consonants(word))\n",
    "        if consonance_consecu_count >= 6:\n",
    "            consonants_consecutive.append(1)\n",
    "        else:\n",
    "            consonants_consecutive.append(0)\n",
    "\n",
    "        # feature 9 \n",
    "        # delete the 1st and last symbol of the string: remaining(2 or more special symbol) = 1; otherwise, = 0\n",
    "        word_removed = subset_word(word)\n",
    "        remove_count = len([r for r in word_removed if r not in vowels or consonants or digits])\n",
    "        if remove_count >=3:\n",
    "            infix_nonalpha.append(1)\n",
    "        else:\n",
    "            infix_nonalpha.append(0)  \n",
    "\n",
    "        # feature 10 bigram\n",
    "        bf = bigram_freq(word, LB)\n",
    "    \n",
    "        lower_tess_tokens = []\n",
    "        #  = word.lower()\n",
    "        lower_tess_tokens.append(word_lower)\n",
    "        \n",
    "        # number of bigrams of the string \n",
    "        n = len(lower_tess_tokens)-1\n",
    "        # bigr: (sum(bf_i)/10000) / n\n",
    "        big = div((sum(bf)/10000),n)\n",
    "        bigram.append(big)\n",
    "\n",
    "        # feature 11 most frequent symbol\n",
    "        i_count = Counter(word).most_common(1)[0][1]\n",
    "        if i_count >=3:\n",
    "            most_freq.append(i_count/L)   # i/l\n",
    "        else:\n",
    "            most_freq.append(0)\n",
    "\n",
    "        # feature 12 Non-alphabetical symbols: nonalpha/total symbols\n",
    "        l1 = len([v for v in word_lower if v in vowels]) + len([c for c in word_lower if c in consonants])\n",
    "        l2_count = L - l1\n",
    "        ratio_l2 = div(l2_count,l1)\n",
    "        l2.append(ratio_l2)  \n",
    "        \n",
    "    ##### construct a feature dataframe for SVM\n",
    "    df1 = pd.DataFrame({'length': length,\n",
    "                        'vowels': vowels_count,\n",
    "                        'consonants': consonants_count,\n",
    "                        'quot v/l': ratio_v_l,\n",
    "                        'quot c/l': ratio_c_l,\n",
    "                        'quot v/c': ratio_v_c,\n",
    "                        'nonalpha': nonalpha,\n",
    "                        'digits': digits,\n",
    "                        'quot d/l': ratio_d_l,\n",
    "                        'lowers': lower_case,\n",
    "                        'uppers': upper_case,\n",
    "                        'quot low/l': ratio_low_l,\n",
    "                        'quot up/l': ratio_up_l,\n",
    "                        'cons occur': consecutive_occur,\n",
    "                        'quot cons/l': ratio_sym_l,\n",
    "                        'quot la': ratio_la,\n",
    "                        'consonants_secut': consonants_consecutive,\n",
    "                        'fix_nonalpha': infix_nonalpha,\n",
    "                        'bigr': bigram,\n",
    "                        'most_freq' : most_freq,\n",
    "                        'l2': l2})\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4qm7cN9RDqv"
   },
   "outputs": [],
   "source": [
    "X_feature_train = feature_matrix(X_train)\n",
    "X_feature_test = feature_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "VD1UsNAORDq0",
    "outputId": "34b6487b-0fba-43ae-f87b-271c4e046e5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shengweihuang/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')  \n",
    "svclassifier.fit(X_feature_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GuAblCa7eM0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3d9bc06f5374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'svm_model.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from google.colab import files\n",
    "\n",
    "filename = 'svm_model.pkl'\n",
    "pickle.dump(svclassifier, open(filename, 'wb'))\n",
    "files.download('svm_model.pkl')\n",
    "# Can load the model from output or upload to colab\n",
    "# svm_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSwqJ770RDq4"
   },
   "outputs": [],
   "source": [
    "#to predict\n",
    "y_pred = svclassifier.predict(X_feature_test)  \n",
    "# y_pred = svm_model.predict(X_feature_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1127
    },
    "colab_type": "code",
    "id": "XGh8N69KRDq8",
    "outputId": "f592fa21-538d-44c6-baaf-d065761ff038"
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame({'tokens_tesseract':X_test,\n",
    "                          'Predict_by_SVM': y_pred})\n",
    "print(df_output[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "LgJ-6B2tRDrC",
    "outputId": "e4512c20-ce44-4c10-e203-e21bba73e5b5"
   },
   "outputs": [],
   "source": [
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7ezVSAnaity"
   },
   "source": [
    "## Error correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CktPwLtalAr"
   },
   "source": [
    "Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates. Here, we implement the positional binary digram method in the first reference paper. (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564}{positional binary digram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VEfK86-1Dd5"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fn7sOOYDapFz"
   },
   "outputs": [],
   "source": [
    "def keep_alphabet(tokens):\n",
    "  # only retain alphabet\n",
    "  out = []\n",
    "  for l in tokens:\n",
    "      l = l.lower()\n",
    "      if l in set('abcdefghijklmnopqrstuvwxyz '):\n",
    "          out.append(l)\n",
    "  return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOWpBGeSeAzz"
   },
   "outputs": [],
   "source": [
    "# Replace a postion of a string\n",
    "def replace_str_index(text,index=0,replacement=''):\n",
    "    return '%s%s%s'%(text[:index],replacement,text[index+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBwd4B3oh6LL"
   },
   "outputs": [],
   "source": [
    "# Define the main correction function\n",
    "def correction(word, digrams):\n",
    "  detect = 0\n",
    "  beta = []\n",
    "  # Find the error positions\n",
    "  for each in digrams:\n",
    "    matrix = digrams[each]\n",
    "    if matrix[string.ascii_lowercase.index(word[each[0]])][string.ascii_lowercase.index(word[each[1]])] == 0:\n",
    "      detect += 1\n",
    "      beta_each = set(each)\n",
    "      if detect == 1:\n",
    "        beta = beta_each\n",
    "      else:\n",
    "        beta = beta.intersection(beta_each)\n",
    "  # print(beta)\n",
    "  # Consider when there is one or two elements in beta\n",
    "  if len(beta) in [1,2]:\n",
    "    choices = 0\n",
    "    v_dict = {}\n",
    "    for i in beta:\n",
    "      v_list = []\n",
    "      position = i\n",
    "      for j in range(len(word)):\n",
    "        alpha_j = string.ascii_lowercase.index(word[j])\n",
    "        if j < position:\n",
    "          vector_j = digrams[(j, position)][alpha_j]\n",
    "          v_list.append(vector_j)\n",
    "        elif j > position:\n",
    "          vector_j = [item[alpha_j] for item in digrams[(position, j)]]\n",
    "          v_list.append(vector_j)\n",
    "      v = v_list[0]\n",
    "      for each in v_list:\n",
    "        v = [a and b for a, b in zip(v, each)]\n",
    "      if sum(v) == 1:\n",
    "        choices += 1\n",
    "        v_dict[i] = v\n",
    "    if choices == 1:\n",
    "      for key in v_dict:\n",
    "        position_chosen = key\n",
    "      word = replace_str_index(word, position_chosen, string.ascii_lowercase[v_dict[position_chosen].index(1)])\n",
    "  return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vr1Bsyh1cPUL"
   },
   "outputs": [],
   "source": [
    "for i in range(len(ground_train)):\n",
    "  ground_train[i] = keep_alphabet(ground_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KztP5VHZcN2J"
   },
   "outputs": [],
   "source": [
    "# Create dictionary of digrams\n",
    "token_by_len = collections.defaultdict(list)\n",
    "digrams_by_len = collections.defaultdict(dict)\n",
    "for w in ground_train:\n",
    "  token_by_len[len(w)].append(w)\n",
    "  \n",
    "#print('Number of words of diffenrent length:')\n",
    "#for key, value in token_by_len.items() :\n",
    "#    print (key, len(value))\n",
    "\n",
    "for length in token_by_len:\n",
    "  for i, j in itertools.combinations(range(length), 2):\n",
    "    key = (i, j)\n",
    "    matrix = [[0] * 26 for _ in range(26)]\n",
    "    for words in token_by_len[length]:\n",
    "      matrix[string.ascii_lowercase.index(words[i])][string.ascii_lowercase.index(words[j])] = 1\n",
    "    digrams_by_len[length][key] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KBWvfZZbVQCd",
    "outputId": "37ca3d07-b0b8-43e7-b024-43ed70a60e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "# Number of 1 in digrams with given length\n",
    "ae = 0\n",
    "for each in digrams_by_len[10][(0,1)]:\n",
    "  ae += sum(each)\n",
    "print(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEmgfeS4KP5R"
   },
   "outputs": [],
   "source": [
    "# Clean the words\n",
    "corrected_test = X_test.copy()\n",
    "for i in range(len(corrected_test)):\n",
    "  corrected_test[i] = keep_alphabet(corrected_test[i])\n",
    "for i in range(len(ground_test)):\n",
    "  ground_test[i] = keep_alphabet(ground_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TSJth-EgTOb"
   },
   "outputs": [],
   "source": [
    "# Make correction\n",
    "for i in range(len(y_pred)):\n",
    "  if y_pred[i] == 1:\n",
    "    word_length = len(corrected_test[i])\n",
    "    if word_length > 1:\n",
    "      digrams_i = digrams_by_len[word_length]\n",
    "      corrected_test[i] = correction(corrected_test[i], digrams_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "towmW8Vkuj3-"
   },
   "outputs": [],
   "source": [
    "y_corrected = []\n",
    "for gt, ct in zip(ground_test, corrected_test):\n",
    "        if gt == ct:\n",
    "            y_corrected.append(0)   # 0 indicates correct\n",
    "        else:\n",
    "            y_corrected.append(1)   # 1 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "P2uE-TeO7bQX",
    "outputId": "57b2e4a2-e157-4aeb-e8ec-52df581fcb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'on', 'and', 'washlngton', '', '', 'report', '', 'reached', 'could', 'cozporatlon', 'the', 'recent', 'vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'mr', 'addltlonal', 'to', 'approach', 'a', 'commlttee', 'are', 'would', 'but']\n",
      "['in', 'on', 'and', 'washington', '', 'it', 'report', '', 'reached', 'could', 'corporation', 'the', 'recent', 'vent', 'session', 'are', 'and', 'able', 'governments', 'commodity', 'the', 'mr', 'additional', 'to', 'approach', 'a', 'committee', 'are', 'would', 'but']\n",
      "['1n', 'on', 'and', 'Washlngton,', '7', '1:', 'Report', '40', 'reached', 'could', 'Cozporatlon', 'the', 'recent', 'Vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'Mr.', 'Addltlonal', 'to', 'approach', 'a', 'Commlttee', 'are:', 'would', 'but']\n"
     ]
    }
   ],
   "source": [
    "# Compare the results\n",
    "print(corrected_test[:30])\n",
    "print(ground_test[:30])\n",
    "print(X_test[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RathHlusCCmv",
    "outputId": "0357315b-8faa-4676-abf4-1093d94898c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12039\n",
      "10362\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_test))\n",
    "print(sum(y_corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVSjkKFiAwdQ"
   },
   "source": [
    "## Performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8QLnVn18-k7"
   },
   "outputs": [],
   "source": [
    "# Create corresponding lists of characters\n",
    "corrected_char = []\n",
    "for each in corrected_test:\n",
    "  corrected_char += each\n",
    "corrected_char\n",
    "\n",
    "ground_char = []\n",
    "for each in ground_test:\n",
    "  ground_char += each\n",
    "\n",
    "tess_char = [] \n",
    "tess_test = X_test.copy()\n",
    "for i in range(len(tess_test)):\n",
    "  tess_test[i] = keep_alphabet(tess_test[i])\n",
    "for each in tess_test:\n",
    "  tess_char += each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RN0kdSp_RoYM"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7swhDLPT-EZK"
   },
   "outputs": [],
   "source": [
    "# Count the number of characters\n",
    "ground_count = []\n",
    "corrected_count = []\n",
    "tess_count = []\n",
    "for each in string.ascii_lowercase:\n",
    "  ground_count.append(ground_char.count(each))\n",
    "  corrected_count.append(corrected_char.count(each))\n",
    "  tess_count.append(tess_char.count(each))\n",
    "ground_corrected_min = np.minimum(ground_count, corrected_count)\n",
    "ground_tess_min = np.minimum(ground_count, tess_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "iH01NQqpAowo",
    "outputId": "b2436256-1bf0-447e-b71d-28f14b99f950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Tesseract  Tesseract_with_postprocessing\n",
      "word_wise_recall           0.604358                       0.659470\n",
      "word_wise_precision        0.604358                       0.659470\n",
      "character_wise_recall      0.912326                       0.917584\n",
      "character_wise_precision   0.945590                       0.951040\n"
     ]
    }
   ],
   "source": [
    "# Make the evaluation table\n",
    "word_recall_tess = 1 - sum(y_test)/len(ground_test)\n",
    "word_recall_corrected = 1 - sum(y_corrected)/len(ground_test)\n",
    "word_precision_tess = 1 - sum(y_test)/len(X_test)\n",
    "word_precision_corrected = 1 - sum(y_corrected)/len(X_test)\n",
    "char_recall_tess = sum(ground_tess_min)/sum(ground_count)\n",
    "char_precision_tess = sum(ground_tess_min)/sum(tess_count)\n",
    "char_recall_corrected = sum(ground_corrected_min)/sum(ground_count)\n",
    "char_precision_corrected = sum(ground_corrected_min)/sum(corrected_count)\n",
    "\n",
    "d = {'Tesseract': [word_recall_tess, word_precision_tess, char_recall_tess, char_precision_tess], \n",
    "     'Tesseract_with_postprocessing': [word_recall_corrected, word_precision_corrected \n",
    "                                       ,char_recall_corrected, char_precision_corrected]}\n",
    "OCR_performance_table = pd.DataFrame(data=d)\n",
    "OCR_performance_table.rename(index={0: 'word_wise_recall', 1:'word_wise_precision', \n",
    "                                    2:'character_wise_recall', 3:'character_wise_precision'}, inplace=True)\n",
    "print(OCR_performance_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHvcr4LMA0VQ"
   },
   "source": [
    "The word-wise recall after postprocessing is 0.6595, significantly better than original Tesseract text. Word-wise precision is the same as recall because of our preprocessing. The character-wise recall and precision are slightly improved to 0.9176 and 0.951."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project4_Detection_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
