{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1YWiD2WA1fN"
   },
   "source": [
    "# Optical character recognition (OCR) \n",
    "Project 4 Group 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rViGVJ0xRK5u"
   },
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0c7XybVBJrI"
   },
   "source": [
    "First of all, we need to detect errors, or incorrectly processed words. Here we extract features according to the paper and use SVM for garbage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3571
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yfF_19MM9yWY",
    "outputId": "c7c5fc2c-6ff1-4bee-b4c4-0814bcd1a27c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "   creating: data/ground_truth/\n",
      "  inflating: data/ground_truth/group1_00000005.txt  \n",
      "  inflating: data/ground_truth/group1_00000010.txt  \n",
      "  inflating: data/ground_truth/group1_00000013.txt  \n",
      "  inflating: data/ground_truth/group1_00000018.txt  \n",
      "  inflating: data/ground_truth/group1_00000021.txt  \n",
      "  inflating: data/ground_truth/group1_00000031.txt  \n",
      "  inflating: data/ground_truth/group1_00000035.txt  \n",
      "  inflating: data/ground_truth/group1_00000043.txt  \n",
      "  inflating: data/ground_truth/group1_00000049.txt  \n",
      "  inflating: data/ground_truth/group1_00000053.txt  \n",
      "  inflating: data/ground_truth/group2_00000004.txt  \n",
      "  inflating: data/ground_truth/group2_00000005.txt  \n",
      "  inflating: data/ground_truth/group2_00000015.txt  \n",
      "  inflating: data/ground_truth/group2_00000016.txt  \n",
      "  inflating: data/ground_truth/group2_00000017.txt  \n",
      "  inflating: data/ground_truth/group2_00000018.txt  \n",
      "  inflating: data/ground_truth/group2_00000034.txt  \n",
      "  inflating: data/ground_truth/group2_00000035.txt  \n",
      "  inflating: data/ground_truth/group2_00000036.txt  \n",
      "  inflating: data/ground_truth/group2_00000037.txt  \n",
      "  inflating: data/ground_truth/group2_00000042.txt  \n",
      "  inflating: data/ground_truth/group2_00000043.txt  \n",
      "  inflating: data/ground_truth/group2_00000047.txt  \n",
      "  inflating: data/ground_truth/group2_00000048.txt  \n",
      "  inflating: data/ground_truth/group2_00000050_1.txt  \n",
      "  inflating: data/ground_truth/group2_00000050_2.txt  \n",
      "  inflating: data/ground_truth/group2_00000055.txt  \n",
      "  inflating: data/ground_truth/group2_00000059.txt  \n",
      "  inflating: data/ground_truth/group2_00000060.txt  \n",
      "  inflating: data/ground_truth/group2_00000061.txt  \n",
      "  inflating: data/ground_truth/group2_00000062.txt  \n",
      "  inflating: data/ground_truth/group2_00000069.txt  \n",
      "  inflating: data/ground_truth/group2_00000070.txt  \n",
      "  inflating: data/ground_truth/group2_00000071.txt  \n",
      "  inflating: data/ground_truth/group2_00000080.txt  \n",
      "  inflating: data/ground_truth/group2_00000081.txt  \n",
      "  inflating: data/ground_truth/group2_00000096.txt  \n",
      "  inflating: data/ground_truth/group2_00000097.txt  \n",
      "  inflating: data/ground_truth/group3_00000043_1.txt  \n",
      "  inflating: data/ground_truth/group3_00000043_2.txt  \n",
      "  inflating: data/ground_truth/group3_00000043_3.txt  \n",
      "  inflating: data/ground_truth/group4_00000003_1.txt  \n",
      "  inflating: data/ground_truth/group4_00000003_2.txt  \n",
      "  inflating: data/ground_truth/group4_00000003_3.txt  \n",
      "  inflating: data/ground_truth/group4_00000003_4.txt  \n",
      "  inflating: data/ground_truth/group4_00000003_5.txt  \n",
      "  inflating: data/ground_truth/group4_00000003_6.txt  \n",
      "  inflating: data/ground_truth/group4_00000003_7.txt  \n",
      "  inflating: data/ground_truth/group4_00000006_1.txt  \n",
      "  inflating: data/ground_truth/group4_00000006_2.txt  \n",
      "  inflating: data/ground_truth/group4_00000006_3.txt  \n",
      "  inflating: data/ground_truth/group4_00000006_4.txt  \n",
      "  inflating: data/ground_truth/group4_00000006_5.txt  \n",
      "  inflating: data/ground_truth/group4_00000006_6.txt  \n",
      "  inflating: data/ground_truth/group4_00000006_7.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_1.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_2.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_3.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_4.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_5.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_6.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_7.txt  \n",
      "  inflating: data/ground_truth/group4_00000009_8.txt  \n",
      "  inflating: data/ground_truth/group4_00000013_1.txt  \n",
      "  inflating: data/ground_truth/group4_00000013_2.txt  \n",
      "  inflating: data/ground_truth/group4_00000013_3.txt  \n",
      "  inflating: data/ground_truth/group4_00000013_4.txt  \n",
      "  inflating: data/ground_truth/group4_00000013_5.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_1.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_10.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_2.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_3.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_4.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_5.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_6.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_7.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_8.txt  \n",
      "  inflating: data/ground_truth/group5_00000003_9.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_1.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_10.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_11.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_12.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_13.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_2.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_3.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_4.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_5.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_6.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_7.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_8.txt  \n",
      "  inflating: data/ground_truth/group5_00000009_9.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_1.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_2.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_3.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_4.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_5.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_6.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_7.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_8.txt  \n",
      "  inflating: data/ground_truth/group5_00000012_9.txt  \n",
      "  inflating: data/README.md          \n",
      "   creating: data/tesseract/\n",
      "  inflating: data/tesseract/group1_00000005.txt  \n",
      "  inflating: data/tesseract/group1_00000010.txt  \n",
      "  inflating: data/tesseract/group1_00000013.txt  \n",
      "  inflating: data/tesseract/group1_00000018.txt  \n",
      "  inflating: data/tesseract/group1_00000021.txt  \n",
      "  inflating: data/tesseract/group1_00000031.txt  \n",
      "  inflating: data/tesseract/group1_00000035.txt  \n",
      "  inflating: data/tesseract/group1_00000043.txt  \n",
      "  inflating: data/tesseract/group1_00000049.txt  \n",
      "  inflating: data/tesseract/group1_00000053.txt  \n",
      "  inflating: data/tesseract/group2_00000004.txt  \n",
      "  inflating: data/tesseract/group2_00000005.txt  \n",
      "  inflating: data/tesseract/group2_00000015.txt  \n",
      "  inflating: data/tesseract/group2_00000016.txt  \n",
      "  inflating: data/tesseract/group2_00000017.txt  \n",
      "  inflating: data/tesseract/group2_00000018.txt  \n",
      "  inflating: data/tesseract/group2_00000034.txt  \n",
      "  inflating: data/tesseract/group2_00000035.txt  \n",
      "  inflating: data/tesseract/group2_00000036.txt  \n",
      "  inflating: data/tesseract/group2_00000037.txt  \n",
      "  inflating: data/tesseract/group2_00000042.txt  \n",
      "  inflating: data/tesseract/group2_00000043.txt  \n",
      "  inflating: data/tesseract/group2_00000047.txt  \n",
      "  inflating: data/tesseract/group2_00000048.txt  \n",
      "  inflating: data/tesseract/group2_00000050_1.txt  \n",
      "  inflating: data/tesseract/group2_00000050_2.txt  \n",
      "  inflating: data/tesseract/group2_00000055.txt  \n",
      "  inflating: data/tesseract/group2_00000059.txt  \n",
      "  inflating: data/tesseract/group2_00000060.txt  \n",
      "  inflating: data/tesseract/group2_00000061.txt  \n",
      "  inflating: data/tesseract/group2_00000062.txt  \n",
      "  inflating: data/tesseract/group2_00000069.txt  \n",
      "  inflating: data/tesseract/group2_00000070.txt  \n",
      "  inflating: data/tesseract/group2_00000071.txt  \n",
      "  inflating: data/tesseract/group2_00000080.txt  \n",
      "  inflating: data/tesseract/group2_00000081.txt  \n",
      "  inflating: data/tesseract/group2_00000096.txt  \n",
      "  inflating: data/tesseract/group2_00000097.txt  \n",
      "  inflating: data/tesseract/group3_00000043_1.txt  \n",
      "  inflating: data/tesseract/group3_00000043_2.txt  \n",
      "  inflating: data/tesseract/group3_00000043_3.txt  \n",
      "  inflating: data/tesseract/group4_00000003_1.txt  \n",
      "  inflating: data/tesseract/group4_00000003_2.txt  \n",
      "  inflating: data/tesseract/group4_00000003_3.txt  \n",
      "  inflating: data/tesseract/group4_00000003_4.txt  \n",
      "  inflating: data/tesseract/group4_00000003_5.txt  \n",
      "  inflating: data/tesseract/group4_00000003_6.txt  \n",
      "  inflating: data/tesseract/group4_00000003_7.txt  \n",
      "  inflating: data/tesseract/group4_00000006_1.txt  \n",
      "  inflating: data/tesseract/group4_00000006_2.txt  \n",
      "  inflating: data/tesseract/group4_00000006_3.txt  \n",
      "  inflating: data/tesseract/group4_00000006_4.txt  \n",
      "  inflating: data/tesseract/group4_00000006_5.txt  \n",
      "  inflating: data/tesseract/group4_00000006_6.txt  \n",
      "  inflating: data/tesseract/group4_00000006_7.txt  \n",
      "  inflating: data/tesseract/group4_00000009_1.txt  \n",
      "  inflating: data/tesseract/group4_00000009_2.txt  \n",
      "  inflating: data/tesseract/group4_00000009_3.txt  \n",
      "  inflating: data/tesseract/group4_00000009_4.txt  \n",
      "  inflating: data/tesseract/group4_00000009_5.txt  \n",
      "  inflating: data/tesseract/group4_00000009_6.txt  \n",
      "  inflating: data/tesseract/group4_00000009_7.txt  \n",
      "  inflating: data/tesseract/group4_00000009_8.txt  \n",
      "  inflating: data/tesseract/group4_00000013_1.txt  \n",
      "  inflating: data/tesseract/group4_00000013_2.txt  \n",
      "  inflating: data/tesseract/group4_00000013_3.txt  \n",
      "  inflating: data/tesseract/group4_00000013_4.txt  \n",
      "  inflating: data/tesseract/group4_00000013_5.txt  \n",
      "  inflating: data/tesseract/group5_00000003_1.txt  \n",
      "  inflating: data/tesseract/group5_00000003_10.txt  \n",
      "  inflating: data/tesseract/group5_00000003_2.txt  \n",
      "  inflating: data/tesseract/group5_00000003_3.txt  \n",
      "  inflating: data/tesseract/group5_00000003_4.txt  \n",
      "  inflating: data/tesseract/group5_00000003_5.txt  \n",
      "  inflating: data/tesseract/group5_00000003_6.txt  \n",
      "  inflating: data/tesseract/group5_00000003_7.txt  \n",
      "  inflating: data/tesseract/group5_00000003_8.txt  \n",
      "  inflating: data/tesseract/group5_00000003_9.txt  \n",
      "  inflating: data/tesseract/group5_00000009_1.txt  \n",
      "  inflating: data/tesseract/group5_00000009_10.txt  \n",
      "  inflating: data/tesseract/group5_00000009_11.txt  \n",
      "  inflating: data/tesseract/group5_00000009_12.txt  \n",
      "  inflating: data/tesseract/group5_00000009_13.txt  \n",
      "  inflating: data/tesseract/group5_00000009_2.txt  \n",
      "  inflating: data/tesseract/group5_00000009_3.txt  \n",
      "  inflating: data/tesseract/group5_00000009_4.txt  \n",
      "  inflating: data/tesseract/group5_00000009_5.txt  \n",
      "  inflating: data/tesseract/group5_00000009_6.txt  \n",
      "  inflating: data/tesseract/group5_00000009_7.txt  \n",
      "  inflating: data/tesseract/group5_00000009_8.txt  \n",
      "  inflating: data/tesseract/group5_00000009_9.txt  \n",
      "  inflating: data/tesseract/group5_00000012_1.txt  \n",
      "  inflating: data/tesseract/group5_00000012_2.txt  \n",
      "  inflating: data/tesseract/group5_00000012_3.txt  \n",
      "  inflating: data/tesseract/group5_00000012_4.txt  \n",
      "  inflating: data/tesseract/group5_00000012_5.txt  \n",
      "  inflating: data/tesseract/group5_00000012_6.txt  \n",
      "  inflating: data/tesseract/group5_00000012_7.txt  \n",
      "  inflating: data/tesseract/group5_00000012_8.txt  \n",
      "  inflating: data/tesseract/group5_00000012_9.txt  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwpNpdFxUqll"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "#import itertools\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gw4gIzwQRDqJ",
    "outputId": "3616cd5c-2bfd-40c3-f14d-181a99982a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294841\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC']\n"
     ]
    }
   ],
   "source": [
    "##### read ground_truth\n",
    "ground_dir = glob.glob(os.path.join(os.getcwd(),'../data/ground_truth','*.txt'))\n",
    "# ground_tokens = []\n",
    "# for gd in ground_dir:\n",
    "#     with open(gd) as ground_file:\n",
    "#         ground_raw = ground_file.read()\n",
    "#         ground_t = ground_raw.split()   \n",
    "#         ground_tokens += ground_t\n",
    "# print(len(ground_tokens))\n",
    "# print(ground_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zoq2wOeFRDqQ"
   },
   "outputs": [],
   "source": [
    "# make sure corresponding files have the same number of lines\n",
    "tess_dir = glob.glob(os.path.join(os.getcwd(),'../data/tesseract','*.txt'))\n",
    "file_name_gd = []\n",
    "file_name_td = []\n",
    "for gd, td in zip(ground_dir, tess_dir):\n",
    "        with open(gd, encoding=\"utf8\") as ground_file:    #, encoding=\"utf8\"\n",
    "            with open(td, encoding=\"utf8\") as tess_file:                \n",
    "                ground_r = list(ground_file.readlines()) \n",
    "                tess_r = list(tess_file.readlines())\n",
    "                if len(tess_r) == len(ground_r):\n",
    "                    file_name_td.append(td)\n",
    "                    file_name_gd.append(gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rz5JErxMRDqU"
   },
   "outputs": [],
   "source": [
    "# make sure all the lines have the same size.\n",
    "# recreate ground_tokens and tess_tokens\n",
    "ground_tokens=[]\n",
    "tess_tokens=[]\n",
    "for gd, td in zip(file_name_gd, file_name_td):\n",
    "        with open(gd) as file1:\n",
    "            with open(td) as file2:\n",
    "                for line1,line2 in zip(file1,file2):\n",
    "                    if len(line1)==len(line2):\n",
    "                        #if length is different, skip that line\n",
    "                        for word1,word2 in zip(line1.split(),line2.split()):\n",
    "                            ground_tokens.append(word1)\n",
    "                            tess_tokens.append(word2)\n",
    "#ground_tokens and tess_tokens are a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xs2RfLUgXj6e",
    "outputId": "c4ace6b0-a38d-42a0-c5fe-0c412f0176eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152143\n",
      "152143\n",
      "['communlcatlons', 'network.', 'Member', 'companles', 'are', 'strongly', 'encouraged', 'to', 'provlde', 'thls', 'needed', 'support.', 'The', 'state', 'advocacy', 'program\"', '1nclud1ng', 'the', 'new', 'CMA/LINC', 'computer', 'network.', 'will', 'be', 'heavlly', '1nvolved', '1n', '1995', '1n', 'the']\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC', 'computer', 'network,', 'will', 'be', 'heavily', 'involved', 'in', '1986', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(tess_tokens[:30])\n",
    "print(ground_tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IYwWhSBlRDqX",
    "outputId": "41f1bf1d-aad9-42e4-8b37-daaf9e123d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152143\n",
      "152143\n"
     ]
    }
   ],
   "source": [
    "print(len(tess_tokens))\n",
    "print(len(ground_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbgdRcXPRDqc"
   },
   "outputs": [],
   "source": [
    "# for every word in tess_tokens, y indicates the correctness of the word\n",
    "y = []\n",
    "for gt, tt in zip(ground_tokens, tess_tokens):\n",
    "        if gt == tt:\n",
    "            y.append(0)   # 0 indicates correct\n",
    "        else:\n",
    "            y.append(1)   # 1 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIxhIzlfUw7r"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tess_tokens)   \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "ground_train = [ground_tokens[i] for i in X_train.index.tolist()]\n",
    "ground_test = [ground_tokens[i] for i in X_test.index.tolist()]\n",
    "X_train = X_train[0].tolist()\n",
    "X_test = X_test[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "e6hxAoaAmSXD",
    "outputId": "5fa0fbc0-a836-4cea-e349-e9a31bd23eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['were', 'and', 'develop', 'to', \"government's\", 'Executlve', '7', 'the', 'Dr.', 'a', 'Commlttee', 'any', 'Sectlon', 'and', '1935,', '15', 'Dow', 'develop', 'Board', 'spoke', '1n1t', 'and', '5.\"', 'wastes.', 'be', 'of', 'Dlvlslon', 'elected', 'Prlmarlly', 'dlfflcult']\n",
      "['were', 'and', 'develop', 'to', \"government's\", 'Executive', '-', 'the', 'Dr.', 'a', 'Committee', 'any', 'Section', 'and', 'Mr.', 'is', 'Dow', 'development', 'Board', 'spoke', 'initial', 'and', 'And', 'wastes.', 'be', 'of', 'Division', 'elected', 'primarily', 'difficult']\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:30])\n",
    "print(ground_train[:30])\n",
    "print(y_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4xL-rP0tgm_"
   },
   "outputs": [],
   "source": [
    "# define character type\n",
    "vowels = list('aeiou')\n",
    "consonants = list(\"bcdfghjklmnpqrstvwxyz\")\n",
    "digits = list('0123456789')\n",
    "\n",
    "def count_up(word):\n",
    "    u = [x for x in word if x.isupper()]\n",
    "    return len(u)\n",
    "\n",
    "def count_low(word):\n",
    "    l = [x for x in word if x.islower()]\n",
    "    return len(l)\n",
    "\n",
    "#consecutive occurrences of the same symbol\n",
    "def count_cons_occur(word):  \n",
    "    max_count=0\n",
    "    count = 0\n",
    "    curr = ''\n",
    "    for c in word:\n",
    "        if curr == c:\n",
    "            count += 1\n",
    "        else:\n",
    "            max_count = max(max_count, count)\n",
    "            curr = c\n",
    "            count = 1\n",
    "    max_count = max(max_count, count)\n",
    "    return(max_count)\n",
    "\n",
    "#consecutive occurrences of the same consonants\n",
    "def count_cons_occur_consonants(word):  \n",
    "    max_count=0\n",
    "    count = 0\n",
    "    curr = ''\n",
    "    for c in word:\n",
    "        if c in consonants:\n",
    "            if curr == c:\n",
    "                count += 1\n",
    "            else:\n",
    "                max_count = max(max_count, count)\n",
    "                curr = c\n",
    "                count = 1\n",
    "    max_count = max(max_count, count)\n",
    "    return(max_count)\n",
    "\n",
    "#extract a subset of word\n",
    "def trim_word(word, from_start=1, from_end=1):\n",
    "    return word[from_start:len(word) - from_end]\n",
    "\n",
    "def unique(list1):  \n",
    "    # intilize a null list \n",
    "    unique_list = []      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x)\n",
    "            \n",
    "def safe_div(x,y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y\n",
    "\n",
    "#return indexs of bigrams in word\n",
    "def get_bigram_freq(word, bi_dict):\n",
    "    word = word.lower()\n",
    "    bf = []\n",
    "    for i in range(len(word)-1):\n",
    "        key = word[i:i+2]\n",
    "        bf.append(bi_dict[key])\n",
    "    return(bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zd39iCOdEfom",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### find LB in ground_truth, prepared for bigram features\n",
    "# ground_tokens of all words in lowercase\n",
    "lower_ground_tokens = []\n",
    "for tk in ground_tokens:\n",
    "    tkl_g = tk.lower()\n",
    "    lower_ground_tokens.append(tkl_g)\n",
    "\n",
    "# A dict of all bigram frequencies\n",
    "bigram_dict = collections.defaultdict(int)\n",
    "for tk_g in lower_ground_tokens:\n",
    "    for i in range(len(tk_g)-1):\n",
    "        key = tk_g[i:i+2]\n",
    "        bigram_dict[key] += 1\n",
    "#bigram_dict is a dictionary of bigram frequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dict['cd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ld7b_1PMDPn"
   },
   "outputs": [],
   "source": [
    "def feature_matrix(X):\n",
    "    #input: a list of words\n",
    "    #Output: a dataframe included features, every feature is a list\n",
    "    \n",
    "    ##### Feature extracting\n",
    "    # feature 1\n",
    "    length = []\n",
    "\n",
    "    # feature 2\n",
    "    vowels_count = []\n",
    "    consonants_count = []\n",
    "    quotients_v_l = []\n",
    "    quotients_c_l = []\n",
    "    quotients_v_c = []\n",
    "\n",
    "    # feature 3\n",
    "    nonalpha = []\n",
    "\n",
    "    # feature 4\n",
    "    digits = []\n",
    "    quotients_d_l = []\n",
    "\n",
    "    # feature 5\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    quotients_low_l = []\n",
    "    quotients_up_l = []\n",
    "    \n",
    "    #feature 6\n",
    "    consecutive_occur = []\n",
    "    quotients_cons_l = []\n",
    "    \n",
    "    #feature 7\n",
    "    quotients_la = [] # total number of vowels, consonants and digits\n",
    "    \n",
    "    #feature 8\n",
    "    consonants_secut = []\n",
    "    \n",
    "    #feature 9\n",
    "    fix_nonalpha = []\n",
    "\n",
    "    # feature bigram\n",
    "    bigr = []\n",
    "\n",
    "    #most frequent symbol\n",
    "    most_freq = []\n",
    "    \n",
    "    #non-alphabetical symbols\n",
    "    l2 = []\n",
    "    \n",
    "    for tk in X:\n",
    "        ### feature 1\n",
    "        l = len(tk)\n",
    "        length.append(l)\n",
    "\n",
    "        ### feature 2\n",
    "        tkl = tk.lower()\n",
    "        v_count = 0\n",
    "        for v in tkl:\n",
    "            if v in vowels:\n",
    "                v_count +=1\n",
    "        vowels_count.append(v_count)\n",
    "\n",
    "        c_count = 0\n",
    "        for c in tkl:\n",
    "            if c in consonants:\n",
    "                c_count +=1\n",
    "        consonants_count.append(c_count)\n",
    "\n",
    "        quotients_v_l.append(v_count/l)\n",
    "        quotients_c_l.append(c_count/l)\n",
    "        quot_v_c = safe_div(v_count,c_count)\n",
    "        quotients_v_c.append(quot_v_c)\n",
    "\n",
    "        ### feature 4\n",
    "        d_count = len([d for d in tk if d in digits])\n",
    "        digits.append(d_count)\n",
    "        quotients_d_l.append(d_count/l)\n",
    "\n",
    "        ### feature 3\n",
    "        s_count = len([s for s in tk if s not in vowels or consonants or digits])\n",
    "        nonalpha.append(s_count)\n",
    "\n",
    "        ### feature 5\n",
    "        low_count = count_low(tk)\n",
    "        lowers.append(low_count)\n",
    "        up_count = count_up(tk)\n",
    "        uppers.append(up_count)\n",
    "        quotients_low_l.append(low_count/l)\n",
    "        quotients_up_l.append(up_count/l)\n",
    "\n",
    "        ### feature 6\n",
    "        cons_occur_count = int(count_cons_occur(tk))\n",
    "        consecutive_occur.append(cons_occur_count)\n",
    "        if cons_occur_count >= 3:\n",
    "            quotients_cons_l.append(cons_occur_count/l)\n",
    "        else:\n",
    "            quotients_cons_l.append(0)\n",
    "\n",
    "        ### feature 7\n",
    "        la = v_count + c_count + d_count\n",
    "        if s_count > la:\n",
    "            quotients_la.append(1)\n",
    "        else:\n",
    "            quotients_la.append(0)\n",
    "\n",
    "        ### feature 8\n",
    "        consonance_cons_cccur_count = int(count_cons_occur_consonants(tk))\n",
    "        if consonance_cons_cccur_count >= 6:\n",
    "            consonants_secut.append(1)\n",
    "        else:\n",
    "            consonants_secut.append(0)\n",
    "\n",
    "        ### feature 9\n",
    "        tk_removed = trim_word(tk)\n",
    "        k_count = len([k for k in tk_removed if k not in vowels or consonants or digits])\n",
    "        if k_count >=3:\n",
    "            fix_nonalpha.append(1)\n",
    "        else:\n",
    "            fix_nonalpha.append(0)  \n",
    "\n",
    "        ### feature 10 bigram\n",
    "        bf = get_bigram_freq(tk, bigram_dict)\n",
    "\n",
    "        lower_tess_tokens = []\n",
    "        tkl_t = tk.lower()\n",
    "        lower_tess_tokens.append(tkl_t)\n",
    "\n",
    "        n = len(set(lower_tess_tokens))\n",
    "        big = (sum(bf)/10000)/n \n",
    "        bigr.append(big)\n",
    "\n",
    "        ### feature 11 most frequent symbol\n",
    "        i_count = Counter(tk).most_common(1)[0][1]\n",
    "        if i_count >=3:\n",
    "            most_freq.append(1)\n",
    "        else:\n",
    "            most_freq.append(0)\n",
    "\n",
    "        ### feature 12 Non-alphabetical symbols: nonalpha/total symbols\n",
    "        l1 = len([v for v in tk.lower() if v in vowels] + [c for c in tk.lower() if c in consonants])\n",
    "        l2_count = l - l1\n",
    "        quot_l2 = safe_div(l2_count,l1)\n",
    "        l2.append(quot_l2)  \n",
    "        \n",
    "    ##### construct a feature dataframe for SVM\n",
    "    df1 = pd.DataFrame({'length': length,\n",
    "                        'vowels': vowels_count,\n",
    "                        'consonants': consonants_count,\n",
    "                        'quot v/l': quotients_v_l,\n",
    "                        'quot c/l': quotients_c_l,\n",
    "                        'quot v/c': quotients_v_c,\n",
    "                        'nonalpha': nonalpha,\n",
    "                        'digits': digits,\n",
    "                        'quot d/l': quotients_d_l,\n",
    "                        'lowers': lowers,\n",
    "                        'uppers': uppers,\n",
    "                        'quot low/l': quotients_low_l,\n",
    "                        'quot up/l': quotients_up_l,\n",
    "                        'cons occur': consecutive_occur,\n",
    "                        'quot cons/l': quotients_cons_l,\n",
    "                        'quot la': quotients_la,\n",
    "                        'consonants_secut': consonants_secut,\n",
    "                        'fix_nonalpha': fix_nonalpha,\n",
    "                        'bigr': bigr,\n",
    "                        'most_freq' : most_freq,\n",
    "                        'l2': l2})\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4qm7cN9RDqv"
   },
   "outputs": [],
   "source": [
    "#it takes about 10 mins\n",
    "X_feature_train = feature_matrix(X_train)\n",
    "X_feature_test = feature_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "VD1UsNAORDq0",
    "outputId": "34b6487b-0fba-43ae-f87b-271c4e046e5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it takes about 20 mins\n",
    "svclassifier = SVC(kernel='rbf')  \n",
    "svclassifier.fit(X_feature_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import pickle\n",
    "filename = '../output/SVM_model.sav'\n",
    "pickle.dump(svclassifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this chunk is for load model\n",
    "import pickle\n",
    "filename = '../output/SVM_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSwqJ770RDq4"
   },
   "outputs": [],
   "source": [
    "#to predict\n",
    "y_pred = svclassifier.predict(X_feature_test)  \n",
    "# y_pred = svm_model.predict(X_feature_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1127
    },
    "colab_type": "code",
    "id": "XGh8N69KRDq8",
    "outputId": "f592fa21-538d-44c6-baaf-d065761ff038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tokens_tesseract  Predict_by_SVM\n",
      "0            Unlted               0\n",
      "1             EPA'S               1\n",
      "2           because               0\n",
      "3            Signed               0\n",
      "4         commlttee               1\n",
      "5               the               0\n",
      "6           percent               0\n",
      "7        accldental               1\n",
      "8              ppm,               1\n",
      "9               the               0\n",
      "10                5               1\n",
      "11               21               1\n",
      "12          'Senate               0\n",
      "13              the               0\n",
      "14              the               0\n",
      "15              The               0\n",
      "16             th.t               1\n",
      "17               at               0\n",
      "18        Congress,               0\n",
      "19              the               0\n",
      "20             Food               1\n",
      "21              EPA               0\n",
      "22         However.               0\n",
      "23              gas               0\n",
      "24               10               1\n",
      "25           actlon               0\n",
      "26            ’anal               0\n",
      "27           report               0\n",
      "28            staff               1\n",
      "29               at               0\n",
      "..              ...             ...\n",
      "70           state,               0\n",
      "71              and               0\n",
      "72              the               0\n",
      "73    demonstratlon               1\n",
      "74               of               0\n",
      "75               M.               0\n",
      "76               1:               1\n",
      "77               L.               0\n",
      "78               or               0\n",
      "79              and               0\n",
      "80             good               0\n",
      "81               or               0\n",
      "82              Bra               0\n",
      "83          fortlgn               1\n",
      "84     governments.               1\n",
      "85               of               0\n",
      "86              and               0\n",
      "87              who               0\n",
      "88              and               0\n",
      "89         Industry               1\n",
      "90             make               0\n",
      "91              the               0\n",
      "92          flgures               0\n",
      "93           needed               0\n",
      "94               of               0\n",
      "95        contacts.               1\n",
      "96       actlvltles               1\n",
      "97          example               0\n",
      "98        synthetlc               1\n",
      "99            needs               0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_output = pd.DataFrame({'tokens_tesseract':X_test,\n",
    "                          'Predict_by_SVM': y_pred})\n",
    "print(df_output[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "LgJ-6B2tRDrC",
    "outputId": "e4512c20-ce44-4c10-e203-e21bba73e5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16269  2140]\n",
      " [ 3491  8529]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.88      0.85     18409\n",
      "          1       0.80      0.71      0.75     12020\n",
      "\n",
      "avg / total       0.81      0.81      0.81     30429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7ezVSAnaity"
   },
   "source": [
    "## Error correction(I didn't run this part--Xishi Chen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CktPwLtalAr"
   },
   "source": [
    "Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates. Here, we implement the positional binary digram method in the first reference paper. (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564}{positional binary digram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VEfK86-1Dd5"
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fn7sOOYDapFz"
   },
   "outputs": [],
   "source": [
    "def keep_alphabet(tokens):\n",
    "  # only retain alphabet\n",
    "  out = []\n",
    "  for l in tokens:\n",
    "      l = l.lower()\n",
    "      if l in set('abcdefghijklmnopqrstuvwxyz '):\n",
    "          out.append(l)\n",
    "  return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOWpBGeSeAzz"
   },
   "outputs": [],
   "source": [
    "# Replace a postion of a string\n",
    "def replace_str_index(text,index=0,replacement=''):\n",
    "    return '%s%s%s'%(text[:index],replacement,text[index+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBwd4B3oh6LL"
   },
   "outputs": [],
   "source": [
    "# Define the main correction function\n",
    "def correction(word, digrams):\n",
    "  detect = 0\n",
    "  beta = []\n",
    "  # Find the error positions\n",
    "  for each in digrams:\n",
    "    matrix = digrams[each]\n",
    "    if matrix[string.ascii_lowercase.index(word[each[0]])][string.ascii_lowercase.index(word[each[1]])] == 0:\n",
    "      detect += 1\n",
    "      beta_each = set(each)\n",
    "      if detect == 1:\n",
    "        beta = beta_each\n",
    "      else:\n",
    "        beta = beta.intersection(beta_each)\n",
    "  # print(beta)\n",
    "  # Consider when there is one or two elements in beta\n",
    "  if len(beta) in [1,2]:\n",
    "    choices = 0\n",
    "    v_dict = {}\n",
    "    for i in beta:\n",
    "      v_list = []\n",
    "      position = i\n",
    "      for j in range(len(word)):\n",
    "        alpha_j = string.ascii_lowercase.index(word[j])\n",
    "        if j < position:\n",
    "          vector_j = digrams[(j, position)][alpha_j]\n",
    "          v_list.append(vector_j)\n",
    "        elif j > position:\n",
    "          vector_j = [item[alpha_j] for item in digrams[(position, j)]]\n",
    "          v_list.append(vector_j)\n",
    "      v = v_list[0]\n",
    "      for each in v_list:\n",
    "        v = [a and b for a, b in zip(v, each)]\n",
    "      if sum(v) == 1:\n",
    "        choices += 1\n",
    "        v_dict[i] = v\n",
    "    if choices == 1:\n",
    "      for key in v_dict:\n",
    "        position_chosen = key\n",
    "      word = replace_str_index(word, position_chosen, string.ascii_lowercase[v_dict[position_chosen].index(1)])\n",
    "  return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vr1Bsyh1cPUL"
   },
   "outputs": [],
   "source": [
    "for i in range(len(ground_train)):\n",
    "  ground_train[i] = keep_alphabet(ground_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KztP5VHZcN2J"
   },
   "outputs": [],
   "source": [
    "# Create dictionary of digrams\n",
    "token_by_len = collections.defaultdict(list)\n",
    "digrams_by_len = collections.defaultdict(dict)\n",
    "for w in ground_train:\n",
    "  token_by_len[len(w)].append(w)\n",
    "  \n",
    "#print('Number of words of diffenrent length:')\n",
    "#for key, value in token_by_len.items() :\n",
    "#    print (key, len(value))\n",
    "\n",
    "for length in token_by_len:\n",
    "  for i, j in itertools.combinations(range(length), 2):\n",
    "    key = (i, j)\n",
    "    matrix = [[0] * 26 for _ in range(26)]\n",
    "    for words in token_by_len[length]:\n",
    "      matrix[string.ascii_lowercase.index(words[i])][string.ascii_lowercase.index(words[j])] = 1\n",
    "    digrams_by_len[length][key] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KBWvfZZbVQCd",
    "outputId": "37ca3d07-b0b8-43e7-b024-43ed70a60e85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "# Number of 1 in digrams with given length\n",
    "ae = 0\n",
    "for each in digrams_by_len[10][(0,1)]:\n",
    "  ae += sum(each)\n",
    "print(ae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEmgfeS4KP5R"
   },
   "outputs": [],
   "source": [
    "# Clean the words\n",
    "corrected_test = X_test.copy()\n",
    "for i in range(len(corrected_test)):\n",
    "  corrected_test[i] = keep_alphabet(corrected_test[i])\n",
    "for i in range(len(ground_test)):\n",
    "  ground_test[i] = keep_alphabet(ground_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TSJth-EgTOb"
   },
   "outputs": [],
   "source": [
    "# Make correction\n",
    "for i in range(len(y_pred)):\n",
    "  if y_pred[i] == 1:\n",
    "    word_length = len(corrected_test[i])\n",
    "    if word_length > 1:\n",
    "      digrams_i = digrams_by_len[word_length]\n",
    "      corrected_test[i] = correction(corrected_test[i], digrams_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "towmW8Vkuj3-"
   },
   "outputs": [],
   "source": [
    "y_corrected = []\n",
    "for gt, ct in zip(ground_test, corrected_test):\n",
    "        if gt == ct:\n",
    "            y_corrected.append(0)   # 0 indicates correct\n",
    "        else:\n",
    "            y_corrected.append(1)   # 1 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "P2uE-TeO7bQX",
    "outputId": "57b2e4a2-e157-4aeb-e8ec-52df581fcb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'on', 'and', 'washlngton', '', '', 'report', '', 'reached', 'could', 'cozporatlon', 'the', 'recent', 'vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'mr', 'addltlonal', 'to', 'approach', 'a', 'commlttee', 'are', 'would', 'but']\n",
      "['in', 'on', 'and', 'washington', '', 'it', 'report', '', 'reached', 'could', 'corporation', 'the', 'recent', 'vent', 'session', 'are', 'and', 'able', 'governments', 'commodity', 'the', 'mr', 'additional', 'to', 'approach', 'a', 'committee', 'are', 'would', 'but']\n",
      "['1n', 'on', 'and', 'Washlngton,', '7', '1:', 'Report', '40', 'reached', 'could', 'Cozporatlon', 'the', 'recent', 'Vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'Mr.', 'Addltlonal', 'to', 'approach', 'a', 'Commlttee', 'are:', 'would', 'but']\n"
     ]
    }
   ],
   "source": [
    "# Compare the results\n",
    "print(corrected_test[:30])\n",
    "print(ground_test[:30])\n",
    "print(X_test[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RathHlusCCmv",
    "outputId": "0357315b-8faa-4676-abf4-1093d94898c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12039\n",
      "10362\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_test))\n",
    "print(sum(y_corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVSjkKFiAwdQ"
   },
   "source": [
    "## Performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8QLnVn18-k7"
   },
   "outputs": [],
   "source": [
    "# Create corresponding lists of characters\n",
    "corrected_char = []\n",
    "for each in corrected_test:\n",
    "  corrected_char += each\n",
    "corrected_char\n",
    "\n",
    "ground_char = []\n",
    "for each in ground_test:\n",
    "  ground_char += each\n",
    "\n",
    "tess_char = [] \n",
    "tess_test = X_test.copy()\n",
    "for i in range(len(tess_test)):\n",
    "  tess_test[i] = keep_alphabet(tess_test[i])\n",
    "for each in tess_test:\n",
    "  tess_char += each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RN0kdSp_RoYM"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7swhDLPT-EZK"
   },
   "outputs": [],
   "source": [
    "# Count the number of characters\n",
    "ground_count = []\n",
    "corrected_count = []\n",
    "tess_count = []\n",
    "for each in string.ascii_lowercase:\n",
    "  ground_count.append(ground_char.count(each))\n",
    "  corrected_count.append(corrected_char.count(each))\n",
    "  tess_count.append(tess_char.count(each))\n",
    "ground_corrected_min = np.minimum(ground_count, corrected_count)\n",
    "ground_tess_min = np.minimum(ground_count, tess_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "iH01NQqpAowo",
    "outputId": "b2436256-1bf0-447e-b71d-28f14b99f950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Tesseract  Tesseract_with_postprocessing\n",
      "word_wise_recall           0.604358                       0.659470\n",
      "word_wise_precision        0.604358                       0.659470\n",
      "character_wise_recall      0.912326                       0.917584\n",
      "character_wise_precision   0.945590                       0.951040\n"
     ]
    }
   ],
   "source": [
    "# Make the evaluation table\n",
    "word_recall_tess = 1 - sum(y_test)/len(ground_test)\n",
    "word_recall_corrected = 1 - sum(y_corrected)/len(ground_test)\n",
    "word_precision_tess = 1 - sum(y_test)/len(X_test)\n",
    "word_precision_corrected = 1 - sum(y_corrected)/len(X_test)\n",
    "char_recall_tess = sum(ground_tess_min)/sum(ground_count)\n",
    "char_precision_tess = sum(ground_tess_min)/sum(tess_count)\n",
    "char_recall_corrected = sum(ground_corrected_min)/sum(ground_count)\n",
    "char_precision_corrected = sum(ground_corrected_min)/sum(corrected_count)\n",
    "\n",
    "d = {'Tesseract': [word_recall_tess, word_precision_tess, char_recall_tess, char_precision_tess], \n",
    "     'Tesseract_with_postprocessing': [word_recall_corrected, word_precision_corrected \n",
    "                                       ,char_recall_corrected, char_precision_corrected]}\n",
    "OCR_performance_table = pd.DataFrame(data=d)\n",
    "OCR_performance_table.rename(index={0: 'word_wise_recall', 1:'word_wise_precision', \n",
    "                                    2:'character_wise_recall', 3:'character_wise_precision'}, inplace=True)\n",
    "print(OCR_performance_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHvcr4LMA0VQ"
   },
   "source": [
    "The word-wise recall after postprocessing is 0.6595, significantly better than original Tesseract text. Word-wise precision is the same as recall because of our preprocessing. The character-wise recall and precision are slightly improved to 0.9176 and 0.951."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project4_Detection_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
