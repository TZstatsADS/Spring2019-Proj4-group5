{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1YWiD2WA1fN"
   },
   "source": [
    "# Optical character recognition (OCR) \n",
    "Project 4 Group 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rViGVJ0xRK5u"
   },
   "source": [
    "## Error Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0c7XybVBJrI"
   },
   "source": [
    "First of all, we need to detect errors, or incorrectly processed words. Here we extract features according to the paper and use SVM for garbage detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwpNpdFxUqll"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import itertools\n",
    "#import itertools\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gw4gIzwQRDqJ",
    "outputId": "3616cd5c-2bfd-40c3-f14d-181a99982a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294841\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC']\n"
     ]
    }
   ],
   "source": [
    "##### read ground_truth\n",
    "ground_dir = glob.glob(os.path.join(os.getcwd(),'../data/ground_truth','*.txt'))\n",
    "ground_tokens = []\n",
    "for gd in ground_dir:\n",
    "    with open(gd) as ground_file:\n",
    "        ground_raw = ground_file.read()\n",
    "        ground_t = ground_raw.split()   \n",
    "        ground_tokens += ground_t\n",
    "print(len(ground_tokens))\n",
    "print(ground_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zoq2wOeFRDqQ"
   },
   "outputs": [],
   "source": [
    "tess_dir = glob.glob(os.path.join(os.getcwd(),'../data/tesseract','*.txt'))\n",
    "#ground_dir = glob.glob(os.path.join(os.getcwd(),'data/ground_truth','*.txt'))\n",
    "file_name_gd = []\n",
    "file_name_td = []\n",
    "for gd, td in zip(ground_dir, tess_dir):\n",
    "        with open(gd, encoding=\"utf8\") as ground_file:    #, encoding=\"utf8\"\n",
    "            with open(td, encoding=\"utf8\") as tess_file:                \n",
    "                ground_r = list(ground_file.readlines()) \n",
    "                tess_r = list(tess_file.readlines())\n",
    "                if len(tess_r) == len(ground_r):\n",
    "                    file_name_td.append(td)\n",
    "                    file_name_gd.append(gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rz5JErxMRDqU"
   },
   "outputs": [],
   "source": [
    "# make sure all the lines have the same size.\n",
    "ground_tokens=[]\n",
    "tess_tokens=[]\n",
    "for gd, td in zip(file_name_gd, file_name_td):\n",
    "        with open(gd) as file1:\n",
    "            with open(td) as file2:\n",
    "                for line1,line2 in zip(file1,file2):\n",
    "                    if len(line1)==len(line2):\n",
    "                        for word1,word2 in zip(line1.split(),line2.split()):\n",
    "                            ground_tokens.append(word1)\n",
    "                            tess_tokens.append(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xs2RfLUgXj6e",
    "outputId": "c4ace6b0-a38d-42a0-c5fe-0c412f0176eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['communlcatlons', 'network.', 'Member', 'companles', 'are', 'strongly', 'encouraged', 'to', 'provlde', 'thls', 'needed', 'support.', 'The', 'state', 'advocacy', 'program\"', '1nclud1ng', 'the', 'new', 'CMA/LINC', 'computer', 'network.', 'will', 'be', 'heavlly', '1nvolved', '1n', '1995', '1n', 'the']\n",
      "['communications', 'network.', 'Member', 'companies', 'are', 'strongly', 'encouraged', 'to', 'provide', 'this', 'needed', 'support.', 'The', 'state', 'advocacy', 'program*', 'including', 'the', 'new', 'CMA/LINC', 'computer', 'network,', 'will', 'be', 'heavily', 'involved', 'in', '1986', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "print(tess_tokens[:30])\n",
    "print(ground_tokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "IYwWhSBlRDqX",
    "outputId": "41f1bf1d-aad9-42e4-8b37-daaf9e123d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152143\n",
      "152143\n"
     ]
    }
   ],
   "source": [
    "print(len(tess_tokens))\n",
    "print(len(ground_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbgdRcXPRDqc"
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "for gt, tt in zip(ground_tokens, tess_tokens):\n",
    "        if gt == tt:\n",
    "            y.append(0)   # 0 indicates correct\n",
    "        else:\n",
    "            y.append(1)   # 1 indicates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIxhIzlfUw7r"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(tess_tokens)   \n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "ground_train = [ground_tokens[i] for i in X_train.index.tolist()]\n",
    "ground_test = [ground_tokens[i] for i in X_test.index.tolist()]\n",
    "X_train = X_train[0].tolist()\n",
    "X_test = X_test[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "e6hxAoaAmSXD",
    "outputId": "5fa0fbc0-a836-4cea-e349-e9a31bd23eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Attachment', 'the', '1n', 'force', 'a', 'complex.', 'proposed', 'incoming', 'NO', 'trllllon', 'August,', 'New', 'Durlng', 'over', 'focus', 'and', 'credlt', 'Manufacturers', 'and', 'Agreement', 'as', 'trenches,', 'for', 'on', 'Page', 'testlfled', 'that', '15', 'admlnlstratlon']\n",
      "['The', 'Attachment', 'the', 'in', 'force', 'a', 'complex.', 'proposed', 'president', 'NO', 'trillion', 'August,', 'New', 'During', 'over', 'focus', 'and', 'credit', 'Manufacturers', 'and', 'Agreement', 'as', 'trenches,', 'for', 'on', 'Page', 'testified', 'that', 'is', 'administration']\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:30])\n",
    "print(ground_train[:30])\n",
    "print(y_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4xL-rP0tgm_"
   },
   "outputs": [],
   "source": [
    "# define character type\n",
    "vowels = list('aeiou')\n",
    "consonants = list(\"bcdfghjklmnpqrstvwxyz\")\n",
    "digits = list('0123456789')\n",
    "\n",
    "def count_up(word):\n",
    "    u = [x for x in word if x.isupper()]\n",
    "    return len(u)\n",
    "\n",
    "def count_low(word):\n",
    "    l = [x for x in word if x.islower()]\n",
    "    return len(l)\n",
    "\n",
    "# count the number of consecutive of the same symbol (if greater than 3 => garbadge)\n",
    "def count_cons_occur(word):\n",
    "    max_occ=0\n",
    "    previous=None\n",
    "    temp=1\n",
    "    for i in word:\n",
    "        if i==previous:\n",
    "            temp=temp+1\n",
    "        else:\n",
    "            max_occ=max(max_occ,temp)\n",
    "            temp=1\n",
    "        previous=i\n",
    "    return(max_occ)\n",
    "\n",
    "#  count the number of consecutive of the consonants(if greater than 6 => garbadge)\n",
    "def count_cons_occur_consonants(word):\n",
    "    max_occ=0\n",
    "    temp=0\n",
    "    for i in word:\n",
    "        if i in consonants:\n",
    "            temp=temp+1\n",
    "        else:\n",
    "            max_occ=max(max_occ,temp)\n",
    "            temp=0\n",
    "    return(max_occ)\n",
    "\n",
    "# get the subset of the word: delete the 1st and last word \n",
    "def subset_word(word):\n",
    "    return word[1:len(word) - 1]\n",
    "\n",
    "#  do the division in case of  prevent from divisor = 0        \n",
    "def div(x,y):\n",
    "    if y != 0:\n",
    "        return x/y\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zd39iCOdEfom",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the bigram frequency for the particular word (return the array of each bf_i)\n",
    "def bigram_freq(word, bi_dict):\n",
    "    word = word.lower()\n",
    "    bf = []\n",
    "    for i in range(len(word)-1):\n",
    "        key = word[i:i+2]\n",
    "        bf.append(bi_dict[key])\n",
    "    return(bf)\n",
    "\n",
    "# Form the list of letter bigrams from our ground truth dataset -- bi_dict\n",
    "# First we normalized all tokens to the lowercase \n",
    "lower_case_words = []\n",
    "for word in ground_tokens:\n",
    "    lower_word = word.lower()\n",
    "    lower_case_words.append(lower_word)\n",
    "\n",
    "# Secondly, we form the LB from our ground_truth dataset \n",
    "# create a dictionary-like collection: key: bigrams value: frequency \n",
    "LB = collections.defaultdict(int)\n",
    "for word in lower_case_words:\n",
    "    for i in range(len(word)-1):\n",
    "        key = word[i:i+2]\n",
    "        LB[key] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ld7b_1PMDPn"
   },
   "outputs": [],
   "source": [
    "def feature_matrix(data):\n",
    "    ##### Feature extracting\n",
    "    \n",
    "    # feature 1 : length of the each words\n",
    "    length = []\n",
    "\n",
    "    # feature 2: number of each vowels and consonants; and the corresponding ratio\n",
    "    vowels_count = []\n",
    "    consonants_count = []\n",
    "    ratio_v_l = []\n",
    "    ratio_c_l = []\n",
    "    ratio_v_c = []\n",
    "\n",
    "    # feature 3: number of special symbols: not vowels, not consonants and not digits \n",
    "    nonalpha = []\n",
    "\n",
    "    # feature 4: number of digits and the ratio bewteen number of digit and length of the words \n",
    "    digits = []\n",
    "    ratio_d_l = []\n",
    "\n",
    "    # feature 5: number of lowercase chars, uppercase chars and corresponding ratio \n",
    "    lower_case = []\n",
    "    upper_case = []\n",
    "    ratio_low_l = []\n",
    "    ratio_up_l = []\n",
    "    \n",
    "    # feature 6: number of consecutive occurances of the same symbol and the ratio between it and the length\n",
    "    consecutive_occur = []\n",
    "    ratio_sym_l = []\n",
    "    \n",
    "    # feature 7: number of all alpha(words) and numbers in the words and the ratio between it and the length\n",
    "    # total number of vowels, consonants and digits\n",
    "    ratio_la = [] \n",
    "    \n",
    "    # feature 8: number of consecutive consonants in the word\n",
    "    consonants_consecutive = []\n",
    "    \n",
    "    # feature 9: delete the 1st and the last symbol and then check the number of non alpha - number  character \n",
    "    infix_nonalpha = []\n",
    "\n",
    "    # feature 10: bigram frequency \n",
    "    bigram = []\n",
    "    \n",
    "    # feature 11: number of most frequent symbols (including alpha,numbers, special symbols)\n",
    "    most_freq = []\n",
    "    \n",
    "    # feature 12: number of non-alpha symbols: length(words) - length(alpha)\n",
    "    l2 = []\n",
    "    \n",
    "    \n",
    "        # feature 1:\n",
    "    for word in data:\n",
    "        L= len(word)\n",
    "        length.append(L)\n",
    "\n",
    "        # feature 2: vowels\n",
    "        word_lower = word.lower()\n",
    "        v_count = len([v for v in word if v in vowels])\n",
    "        vowels_count.append(v_count)\n",
    "        \n",
    "        # feature 2: consonants\n",
    "        c_count = len([c for c in word if c in consonants])\n",
    "        consonants_count.append(c_count)\n",
    "   \n",
    "        ratio_v_l.append(v_count/L)\n",
    "        ratio_c_l.append(c_count/L)\n",
    "        ratio_vc = div(v_count,c_count)\n",
    "        ratio_v_c.append(ratio_vc)\n",
    "\n",
    "        # feature 3\n",
    "        symbol_count = len([s for s in word if s not in vowels or consonants or digits])\n",
    "        nonalpha.append(symbol_count)\n",
    "     \n",
    "        # feature 4\n",
    "        d_count = len([d for d in word if d in digits])\n",
    "        digits.append(d_count)\n",
    "        ratio_d_l.append(d_count/L)\n",
    "        \n",
    "        # feature 5\n",
    "        low_count = count_low(word)\n",
    "        lower_case.append(low_count)\n",
    "        upper_count = count_up(word)\n",
    "        upper_case.append(upper_count)\n",
    "\n",
    "        ratio_low_l.append(low_count/L)\n",
    "        ratio_up_l.append(upper_count/L)\n",
    "\n",
    "        # feature 6\n",
    "        cons_occur_count = int(count_cons_occur(word))\n",
    "        consecutive_occur.append(cons_occur_count)\n",
    "        if cons_occur_count >= 3:\n",
    "            ratio_sym_l.append(cons_occur_count/L)\n",
    "        else:\n",
    "            ratio_sym_l.append(0)\n",
    "\n",
    "        # feature 7\n",
    "        la = L-symbol_count\n",
    "        if symbol_count > la:\n",
    "            ratio_la.append(1)\n",
    "        else:\n",
    "            ratio_la.append(0)\n",
    "\n",
    "        # feature 8\n",
    "        consonance_consecu_count = int(count_cons_occur_consonants(word))\n",
    "        if consonance_consecu_count >= 6:\n",
    "            consonants_consecutive.append(1)\n",
    "        else:\n",
    "            consonants_consecutive.append(0)\n",
    "\n",
    "        # feature 9 \n",
    "        # delete the 1st and last symbol of the string: remaining(2 or more special symbol) = 1; otherwise, = 0\n",
    "        word_removed = subset_word(word)\n",
    "        remove_count = len([r for r in word_removed if r not in vowels or consonants or digits])\n",
    "        if remove_count >=3:\n",
    "            infix_nonalpha.append(1)\n",
    "        else:\n",
    "            infix_nonalpha.append(0)  \n",
    "\n",
    "        # feature 10 bigram\n",
    "        bf = bigram_freq(word, LB)\n",
    "    \n",
    "        lower_tess_tokens = []\n",
    "        #  = word.lower()\n",
    "        lower_tess_tokens.append(word_lower)\n",
    "        \n",
    "        # number of bigrams of the string \n",
    "        n = len(lower_tess_tokens)-1\n",
    "        # bigr: (sum(bf_i)/10000) / n\n",
    "        big = div((sum(bf)/10000),n)\n",
    "        bigram.append(big)\n",
    "\n",
    "        # feature 11 most frequent symbol\n",
    "        i_count = Counter(word).most_common(1)[0][1]\n",
    "        if i_count >=3:\n",
    "            most_freq.append(i_count/L)   # i/l\n",
    "        else:\n",
    "            most_freq.append(0)\n",
    "\n",
    "        # feature 12 Non-alphabetical symbols: nonalpha/total symbols\n",
    "        l1 = len([v for v in word_lower if v in vowels]) + len([c for c in word_lower if c in consonants])\n",
    "        l2_count = L - l1\n",
    "        ratio_l2 = div(l2_count,l1)\n",
    "        l2.append(ratio_l2)  \n",
    "        \n",
    "        # construct a feature dataframe for SVM\n",
    "    df1 = pd.DataFrame({'length': length,\n",
    "                        'vowels': vowels_count,\n",
    "                        'consonants': consonants_count,\n",
    "                        'quot v/l': ratio_v_l,\n",
    "                        'quot c/l': ratio_c_l,\n",
    "                        'quot v/c': ratio_v_c,\n",
    "                        'nonalpha': nonalpha,\n",
    "                        'digits': digits,\n",
    "                        'quot d/l': ratio_d_l,\n",
    "                        'lowers': lower_case,\n",
    "                        'uppers': upper_case,\n",
    "                        'quot low/l': ratio_low_l,\n",
    "                        'quot up/l': ratio_up_l,\n",
    "                        'cons occur': consecutive_occur,\n",
    "                        'quot cons/l': ratio_sym_l,\n",
    "                        'quot la': ratio_la,\n",
    "                        'consonants_secut': consonants_consecutive,\n",
    "                        'fix_nonalpha': infix_nonalpha,\n",
    "                        'bigr': bigram,\n",
    "                        'most_freq' : most_freq,\n",
    "                        'l2': l2})\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4qm7cN9RDqv"
   },
   "outputs": [],
   "source": [
    "X_feature_train = feature_matrix(X_train)\n",
    "X_feature_test = feature_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feature_train.to_csv('../output/X_feature_train')\n",
    "X_feature_test.to_csv('../output/X_feature_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "VD1UsNAORDq0",
    "outputId": "34b6487b-0fba-43ae-f87b-271c4e046e5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shengweihuang/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier = SVC(kernel='rbf')  \n",
    "svclassifier.fit(X_feature_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "import pickle\n",
    "filename = '../output/SVM_model.sav'\n",
    "pickle.dump(svclassifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GuAblCa7eM0"
   },
   "outputs": [],
   "source": [
    "#this chunk is for load model\n",
    "import pickle\n",
    "filename = '../output/SVM_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSwqJ770RDq4"
   },
   "outputs": [],
   "source": [
    "#to predict\n",
    "y_pred = svclassifier.predict(X_feature_test)  \n",
    "# y_pred = svm_model.predict(X_feature_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1127
    },
    "colab_type": "code",
    "id": "XGh8N69KRDq8",
    "outputId": "f592fa21-538d-44c6-baaf-d065761ff038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tokens_tesseract  Predict_by_SVM      True word\n",
      "0         countrles               1      countries\n",
      "1           number.               0        number.\n",
      "2              slze               0             of\n",
      "3        membershlp               1     membership\n",
      "4               out               0            out\n",
      "5            relent               0         relent\n",
      "6       asslstance.               1    assistance.\n",
      "7           strong,               1        strong,\n",
      "8           requlre               0       industry\n",
      "9         Congress.               1      Congress.\n",
      "10        producers               1      producers\n",
      "11              the               0            the\n",
      "12          sildeS.               0        slides,\n",
      "13               by               0              5\n",
      "14              You               0            You\n",
      "15               1n               1              a\n",
      "16            would               0          would\n",
      "17        1ndustry.               1      industry.\n",
      "18             give               0            the\n",
      "19               of               0             of\n",
      "20      dlstrlbuted               1    distributed\n",
      "21         Marltlme               1       Maritime\n",
      "22            EEC'S               1          EEC's\n",
      "23        1nd1cated               1      indicated\n",
      "24          greater               0      awareness\n",
      "25       commerclal               1     commercial\n",
      "26               1n               1             in\n",
      "27       1nterested               1     interested\n",
      "28        avaliable               0      available\n",
      "29          produce               0        produce\n",
      "..              ...             ...            ...\n",
      "70          chronlc               1        chronic\n",
      "71          Testlng               1        Testing\n",
      "72              and               0       function\n",
      "73    1nternatlonal               1  international\n",
      "74       reference,               0     reference,\n",
      "75              the               0            the\n",
      "76              the               0            the\n",
      "77       dlscusslng               1            the\n",
      "78       Instltute,               1     Institute,\n",
      "79            terms               1          terms\n",
      "80           should               0         should\n",
      "81                1               1              1\n",
      "82              the               0            the\n",
      "83              for               0            for\n",
      "84             need               0           need\n",
      "85               be               0             be\n",
      "86        seconded,               0      seconded,\n",
      "87             are:               0           are:\n",
      "88             Will               0           will\n",
      "89        hazardous               0      hazardous\n",
      "90    reexamlnatlon               1  reexamination\n",
      "91        Pollutlon               1      Pollution\n",
      "92            OECD.               1          OECD.\n",
      "93           Signer               0         signer\n",
      "94              the               0            the\n",
      "95          related               0        related\n",
      "96            ships               1          ships\n",
      "97           POLICY               0         REVIEW\n",
      "98              now               0            now\n",
      "99       Commlttee,               1     Committee,\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_output = pd.DataFrame({'tokens_tesseract':X_test,\n",
    "                          'Predict_by_SVM': y_pred,'True word':ground_test})\n",
    "print(df_output[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "LgJ-6B2tRDrC",
    "outputId": "e4512c20-ce44-4c10-e203-e21bba73e5b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15747  2376]\n",
      " [ 3675  8631]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84     18123\n",
      "           1       0.78      0.70      0.74     12306\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     30429\n",
      "   macro avg       0.80      0.79      0.79     30429\n",
      "weighted avg       0.80      0.80      0.80     30429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### evaluation\n",
    "#confustion Matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project4_Detection_v4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
